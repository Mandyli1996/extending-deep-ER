\relax 
\citation{elmagarmid-duplicate-record-detection}
\citation{fellegi-theory}
\citation{naumann-introduction}
\citation{bilenko-adaptive}
\citation{fellegi-theory}
\citation{elmagarmid-duplicate-record-detection}
\citation{ebraheem-deep-er}
\citation{ebraheem-deep-er}
\citation{mikolov-distributed}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Machine Learning for Entity Resolution}{1}}
\citation{ebraheem-deep-er}
\citation{yang-distance}
\citation{schroff-facenet}
\citation{mueller-siamese}
\citation{mikolov-distributed}
\citation{pennington-glove}
\citation{mikolov-distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}DeepER for Entity Resolution}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Project Goals}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Outline}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Word Embeddings}{2}}
\citation{pennington-glove}
\citation{ebraheem-deep-er}
\citation{goodfellow-deep-learning}
\citation{goodfellow-deep-learning}
\citation{collobert-natural}
\citation{mandic-recurrent}
\citation{hochreiter-long}
\citation{schuster-bidirectional}
\citation{yang-distance}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Word embeddings}\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:word-embeddings}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural Networks}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Long Short Term Memory}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Manhattan LSTM}{3}}
\citation{mueller-siamese}
\citation{ebraheem-deep-er}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {LSTM}\relax }}{4}}
\newlabel{fig:word-embeddings-lstm}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {MaLSTM}\relax }}{4}}
\newlabel{fig:malstm}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}DeepER}{4}}
\citation{ebraheem-deep-er}
\citation{ramos-using}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {DeepER framework}\relax }}{5}}
\newlabel{fig:deep-er-framework}{{4}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DeepER - Identifying Matches and Non-matches\relax }}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extending DeepER}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Adapting MaLSTM}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Attribute Compositions}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Inverse document frequency}{5}}
\citation{ebraheem-deep-er}
\citation{ebraheem-deep-er}
\citation{chopra-learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Compound Compositions}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Multiple Similarity Metrics}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Supporting Numerical Attributes}{6}}
\citation{benchmark-datasets}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Supporting Null Value Indicators}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Datasets}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Amazon-Google}{7}}
\citation{ebraheem-deep-er}
\citation{pennington-glove}
\citation{ebraheem-deep-er}
\citation{ebraheem-deep-er}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Amazon-Walmart}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}DBLP-Scholar}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Preprocessing}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Validation and Testing}{8}}
\citation{ebraheem-deep-er}
\citation{ebraheem}
\citation{mueller-siamese}
\citation{ebraheem-deep-er}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Model Configuration and Architecture}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Results}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Manhattan LSTM}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}DeepER Extensions}{9}}
\citation{ebraheem-deep-er}
\citation{ebraheem-deep-er}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Extended DeepER variations}: Models with asterisks refer to baseline models with no extensions.\relax }}{10}}
\newlabel{table:deep-er-variations}{{1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {MaLSTM results}\relax }}{10}}
\newlabel{fig:results-malstm}{{5}{10}}
\citation{ebraheem-deep-er}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Varying attribute embedding method}\relax }}{11}}
\newlabel{fig:results-compositions}{{6}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Varying similarities}\relax }}{11}}
\newlabel{fig:results-similarities}{{7}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {DeepER baseline results}\relax }}{11}}
\newlabel{fig:results-avg-t}{{8}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Varying preprocessing method}\relax }}{11}}
\newlabel{fig:results-preprocessing}{{9}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Variations on average-based attribute embeddings}\relax }}{11}}
\newlabel{fig:results-avg}{{10}{11}}
\bibstyle{ieeetr}
\bibdata{final-report}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {The kitchen sink vs. baselines}\relax }}{12}}
\newlabel{fig:results-kitchen-sink}{{11}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {avg-t vs. avg-num-t}\relax }}{12}}
\newlabel{fig:results-final}{{12}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Concluding Remarks}{12}}
\bibcite{elmagarmid-duplicate-record-detection}{1}
\bibcite{fellegi-theory}{2}
\bibcite{naumann-introduction}{3}
\bibcite{bilenko-adaptive}{4}
\bibcite{ebraheem-deep-er}{5}
\bibcite{mikolov-distributed}{6}
\bibcite{yang-distance}{7}
\bibcite{schroff-facenet}{8}
\bibcite{mueller-siamese}{9}
\bibcite{pennington-glove}{10}
\bibcite{goodfellow-deep-learning}{11}
\bibcite{collobert-natural}{12}
\bibcite{mandic-recurrent}{13}
\bibcite{hochreiter-long}{14}
\bibcite{schuster-bidirectional}{15}
\bibcite{ramos-using}{16}
\bibcite{chopra-learning}{17}
\bibcite{benchmark-datasets}{18}
