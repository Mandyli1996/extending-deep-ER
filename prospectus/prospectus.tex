\documentclass{proc}
\usepackage{url}
\usepackage{amssymb}
\usepackage{parskip}


\begin{document}
\title{Applying triplet loss to entity resolution models}

\author{Charissa Ding, Lisa Kim, Derek Zhao}

\maketitle

\section{Abstract / Introduction}

Entity resolution refers to the task of identifying records in a dataset that refer to the same entity across different data sources. It is one of the most crucial stages in data integration, and researchers have spent the past few decades studying various methods in order to make this process both accurate and efficient.

The most fundamental technique used in entity resolution is to calculate similarity metrics between a pair of records and setting a threshold to decide whether the pair is a match~\cite{elmagarmid2007duplicate}. Additional efforts have been built upon this method, including machine-learning mechanisms such as random forest~\cite{varma2017relic} and decision tree ensembles~\cite{yi2017method}, as well as crowdsourcing~\cite{gokhale2014corleone,wang2012crowder}. Some of the ongoing research on the subject utilizes deep learning algorithms~\cite{ebraheem2017deeper} and introduces tree-based adversarial generation [Euegen Wu proposal???]. However, these models typically cannot achieve 100\% accuracy and make incorrect predictions on some records [Euegen Wu proposal???]. 

In recent years, researchers have made significant progress in Face Recognition utilizing a deep convolutional network trained with triplet loss function, setting a new record accuracy of 99.63\%~\cite{schroff2015facenet}. In this prospectus, we propose to apply this method to the entity resolution problem, in hope to improve both accuracy and efficiency.

This problem is relevant because improving accuracy on entity resolution would greatly improve the overall performance of a data integration task, which is one of the key topics in this course.

\section{One Sentence Summary}

We plan to adapt the use of the triplet loss function which has shown success in facial recognition systems to a deep neural network trained on triplets of database records in hopes that such a model achieves better performance than current state of the art techniques for entity resolution, such as random forests.

\section{Audience and Needs}

This project will be of particular value to the academic community, database managers, and data wranglers. If successful, this project would suggest a new avenue for research in entity resolution. Furthermore, a computationally reasonable model that can resolve entities with high accuracy will lead to more efficient matching, more reliable de-duplication procedures, and more accurate table merging. At its most practical, database managers and data wranglers might be saved substantial amounts of time that would otherwise be spent writing rules and manually examining recors.

\section{Approach}

The problem of entity resolution bears some resemblance to that of facial recognition. In both cases, we are less concerned about what a record is or who a face belongs to and more interested in whether two records or two faces are the same. Recent innovations in facial recognition technology uses neural networks trained on a triplet loss function. In a triplet loss network for facial recognition, rather than predicting the class of an input image $X$, the network outputs a multi-dimensional vector embedding $f(X)$ of that image with the constraint that the Euclidean distance between two embeddings of images of two different people is significantly large. That is, let $\mathbb{I}(X)$ be the identity of the person whose face appears in $X$. If $\mathbb{I}(X) = \mathbb{I}(Y)$ and $\mathbb{I}(X) \neq \mathbb{I}(Z)$, then $\left\|f(X) - f(Z) \right\|  \gg \left\|f(X) - f(Y) \right\|$.

A training instance for such a network consists of a triplet of three images $(A, P, N)$, an anchor, positive, and negative image, respectively, that satisfies $\mathbb{I}(A) = \mathbb{I}(P)$ and $\mathbb{I}(A) \neq \mathbb{I}(N)$. Given said triplet, the loss that they incur on a network is $\mathcal{L}(A, D, P) = \max(0, \left\|f(A) - f(P) \right\|^2 - \left\|f(A) - f(N) \right\|^2 + \alpha)$, where $\alpha$ is a threshold hyperparameter defined prior to training.

The above loss function has proven successful for face recognition, and we believe it can be applied similarly to entity resolution. To generate triplets,  we will use a \emph{discriminator model} to predict whether pairs of records refer to the same entity. The mispredictions of the discriminator model will either consist of false positives which can be used as the $A, N$ records within a triplet or false negatives which can be used as the $A, P$  records within a triplet. Ideally, we would like to show that given an initial discriminator model, using its mispredictions as training examples for a triplet loss network results in a neural model that outperforms the discriminator.

\section{(Best Case) Impact}

We can show that the algorithm produces better results than the state-of-the-art approach and scales well to larger datasets.

\section{Milestones}

\begin{enumerate}
  \item Identify datasets tested in previous methodologies that are computational feasible to train on and that we can use as benchmarks.
  \item Develop an adversarial procedure for generating triplets. This will entail developing a tree-based ensemble and corresponding featurization procedures to generate false positives and false negatives. The accuracy of this procedure may also serve as a benchmark.
  \item Implement a neural network based on the triplet loss function and compare against benchmark used to generate "hard" negatives.
  \item Develop data augmentation procedures to generate positive pairs and  improve the performance of the triplet loss network.
  \item Run experiments on various datasets and compare results with current widespread methods.
\end{enumerate}

\section{Obstacles}
\begin{quote}
\emph{Major obstacles are situations where we would consider \textbf{killing} the project. 
Minor obstacles are situations that would delay the project or increase the overall cost in energy, time, people, and money.}
\end{quote}

\subsection{Major obstacles} 

\begin{itemize}
  \item If we cannot show that a triplet loss network can at the very least improve on the performance of the initial model used to generate false positives and negatives, it is highly unlikely that said network can compare favorably to the state of the art.
\end{itemize}

\subsection{Minor obstacles}

\begin{itemize}
  \item The original purpose of the triplet loss function was to enable facial recognition systems to compare new images of faces against a database of faces the system has already trained on. In other words, a facial recognition system is responsible for determining whether a new face is the same as one it has been trained to recognize, but it is not responsible for determining whether two new faces are the same. It is possible the triplet loss function is effective for only the former and not the latter, in which case a triplet loss network would require significantly more training data and computational resources to be effective.
  \item A typical dataset or even pair of datasets with duplicate entities largely consists of negative pairings. While an initial model such as a random forest can be used to pick out negative pairings most effective for a tripet loss network, we may be constrained by the number of positive pairings, in which case we will need to devise data augmentation methods to generate additional positive pairings to increase the number of training triplets.
  \item We may discover that the neural network architecture needed to effectively embed records as vectors is prohibitively large for the computational resources available. If so, we should consider training on datasets with fewer attributes or using simpler benchmark models for initially detecting false positives and negatives.
\end{itemize}


\section{Additional Resources}

\begin{itemize}
  \item Some computational time to run our neural network algorithm.
  \item Access to a GPU-enabled server in order to perform parallel processing in the neural network model. 
\end{itemize}
 
\section{Literature Review}

\begin{itemize}
\item \emph{Background for the project:} This study applies recent advances in the facial recognition to the problem of entity solution.

\item \emph{Work the project relies and builds on: } Some works on face recognition have introduced the use of triplet loss function to match the people, which have turned out  successful. We could take their idea and apply it to entity matching. Also, the study in DeepER has suggested word embedding as a metric to compute the string similarities.

\item \emph{Direct competitors: } As mentioned above, the study in DeepER used word embedding and deep learning to compute the string similarities and train the model. The researchers of the study has used Stochastic Gradient Descent with back propogation, and their model has outperformed the current state-of-art ML.

\item \emph{Alternatives to achieve the broader goal: } hybrid human-machine workflow and tree-based adversarial generation can be alternatives to perform entity matching more efficiently, reliably, and robustly, but they do not advance the current machine learning mechanism that is commonly used for entity matching.

 \end{itemize}


\section{Define Success}
\begin{quote}
\emph{When / How do you know if you have succeeded in this project?
In other words, what is the minimum finding that would make this project a success and publishable?}
\end{quote}

Due to novelty of the idea, whether or not this neural network model can outperform current benchmarks, it is worthwhile to explore and potentially provide reference to future research in the similar direction.

\bibliographystyle{abbrv}
\bibliography{prospectus}


\end{document}
