\documentclass{proc}
\usepackage{url}
\usepackage{amssymb}
\usepackage{parskip}


\begin{document}
\title{Applying triplet loss to entity resolution models}

\author{Charissa Ding, Lisa Kim, Derek Zhao}

\maketitle

\section{Abstract / Introduction}

Entity resolution refers to the task of identifying records in a dataset that refer to the same entity across different data sources. It is one of the most crucial stages in data integration, and researchers have spent the past few decades studying various methods in order to make this process both accurate and efficient.

The most fundamental technique used in entity resolution is to manually design and calculate similarity metrics between a pair of records and setting a threshold to decide whether the pair is a match~\cite{elmagarmid2007duplicate}. Additional efforts have been built upon this method, including machine-learning algorithms such as decision tree ensembles~\cite{varma2017relic, yi2017method}, as well as crowdsourcing~\cite{gokhale2014corleone,wang2012crowder}. Some of the most recent research efforts on the subject explore techniques such as deep learning algorithms~\cite{ebraheem2017deeper} and tree-based adversarial generation. While these models can achieve near-perfect accuracy on datasets that are well structured and have little noise, they struggle to perform well with datasets that are noisy and have unstructured attributes~\cite{ebraheem2017deeper}.

In recent years, researchers have made significant progress in face recognition utilizing a deep convolutional network trained with triplet loss function, setting a new record accuracy of 99.63\% on the widely used Labeled Faces in the Wild (LFW) dataset with more than 13,000 face images~\cite{schroff2015facenet}. In this prospectus, we propose to apply this method to the entity resolution problem, aiming to build a model that is both accurate and robust as the level of complexity and amount of noise grow in a given dataset.

This problem is relevant because improving accuracy on entity resolution would greatly improve the overall performance of a data integration task, which is one of the key topics in this course.

\section{One Sentence Summary}

We plan to adapt the use of the triplet loss function which has shown success in facial recognition systems to a deep neural network trained on triplets of database records in hopes that such a model achieves better performance than current state of the art techniques for entity resolution, such as random forests.

\section{Audience and Needs}

This project will be of particular value to the academic community, database managers, and data wranglers. If successful, this project would suggest a new avenue for research in entity resolution. Furthermore, a computationally reasonable model that can resolve entities with high accuracy will lead to more efficient matching, more reliable de-duplication procedures, and more accurate table merging. At its most practical, database managers and data wranglers might be saved substantial amounts of time that would otherwise be spent writing rules and manually examining recors.

\section{Approach}

The problem of entity resolution bears some resemblance to that of facial recognition. In both cases, we are less concerned about what a record is or who a face belongs to and more interested in whether two records or two faces are the same. Recent innovations in facial recognition technology uses neural networks trained on a triplet loss function. In a triplet loss network for facial recognition, rather than predicting the class of an input image $X$, the network outputs a multi-dimensional vector embedding $f(X)$ of that image with the constraint that the Euclidean distance between two embeddings of images of two different people is significantly large. That is, let $\mathbb{I}(X)$ be the identity of the person whose face appears in $X$. If $\mathbb{I}(X) = \mathbb{I}(Y)$ and $\mathbb{I}(X) \neq \mathbb{I}(Z)$, then $\left\|f(X) - f(Z) \right\|  \gg \left\|f(X) - f(Y) \right\|$.

A training instance for such a network consists of a triplet of three images $(A, P, N)$, an anchor, positive, and negative image, respectively, that satisfies $\mathbb{I}(A) = \mathbb{I}(P)$ and $\mathbb{I}(A) \neq \mathbb{I}(N)$. Given said triplet, the loss that they incur on a network is $\mathcal{L}(A, D, P) = \max(0, \left\|f(A) - f(P) \right\|^2 - \left\|f(A) - f(N) \right\|^2 + \alpha)$, where $\alpha$ is a threshold hyperparameter defined prior to training.

The above loss function has proven successful for face recognition, and we believe it can be applied similarly to entity resolution. To generate triplets,  we will use a \emph{discriminator model} to predict whether pairs of records refer to the same entity. The mispredictions of the discriminator model will either consist of false positives which can be used as the $A, N$ records within a triplet or false negatives which can be used as the $A, P$  records within a triplet. Ideally, we would like to show that given an initial discriminator model, using its mispredictions as training examples for a triplet loss network results in a neural model that outperforms the discriminator.

\section{(Best Case) Impact}

We can show that the algorithm produces more accurate results than state-of-the-art ML and crowd approaches, and also exceeds our direct competitor the deepER algorithm, which on average have a 0.6 to 0.8 F-measure on challenging and noisy datasets~\cite{ebraheem2017deeper}. In addition, our model should also preserve fast runtime and does not require prohibitive amounts of training data. 

\section{Milestones}

\begin{enumerate}
  \item Identify datasets tested in previous methodologies that are computational feasible to train on and that we can use as benchmarks.
  \item Develop an adversarial procedure for generating triplets. This will entail developing a tree-based ensemble and corresponding featurization procedures to generate false positives and false negatives. The accuracy of this procedure may also serve as a benchmark.
  \item Implement a neural network based on the triplet loss function and compare against benchmark used to generate "hard" negatives.
  \item Develop data augmentation procedures to generate positive pairs and  improve the performance of the triplet loss network.
  \item Run experiments on various datasets and compare results with current widespread methods.
\end{enumerate}

\section{Obstacles}
\begin{quote}
\emph{Major obstacles are situations where we would consider \textbf{killing} the project. 
Minor obstacles are situations that would delay the project or increase the overall cost in energy, time, people, and money.}
\end{quote}

\subsection{Major obstacles} 

\begin{itemize}
  \item If we cannot show that a triplet loss network can at the very least improve on the performance of the initial model used to generate false positives and negatives, it is highly unlikely that said network can compare favorably to the state of the art.
\end{itemize}

\subsection{Minor obstacles}

\begin{itemize}
  \item The original purpose of the triplet loss function was to enable facial recognition systems to compare new images of faces against a database of faces the system has already trained on. In other words, a facial recognition system is responsible for determining whether a new face is the same as one it has been trained to recognize, but it is not responsible for determining whether two new faces are the same. It is possible the triplet loss function is effective for only the former and not the latter, in which case a triplet loss network would require significantly more training data and computational resources to be effective.
  \item A typical dataset or even pair of datasets with duplicate entities largely consists of negative pairings. While an initial model such as a random forest can be used to pick out negative pairings most effective for a tripet loss network, we may be constrained by the number of positive pairings, in which case we will need to devise data augmentation methods to generate additional positive pairings to increase the number of training triplets.
  \item We may discover that the neural network architecture needed to effectively embed records as vectors is prohibitively large for the computational resources available. If so, we should consider training on datasets with fewer attributes or using simpler benchmark models for initially detecting false positives and negatives.
\end{itemize}


\section{Additional Resources}

\begin{itemize}
  \item Some computational time to run our neural network algorithm.
  \item Access to a GPU-enabled server in order to perform parallel processing in the neural network model. 
\end{itemize}
 
\section{Literature Review}

\begin{itemize}
\item This study builds on prior work in entity matching ~\cite{elmagarmid2007duplicate}, particularly the use of supervised learning to differentiate matching and non-matching record pairs ~\cite{kopcke2010evaluation}.

\item Some preliminary work on embedded representations of attributes and entire records suggest a new approach towards calculating the similarity between two records ~\cite{ebraheem2017deeper}. We will build on this direction and borrow the use of the triplet loss function developed for facial recognition ~\cite{schroff2015facenet}.

\item We are not aware of other studies applying distance metric learning such as triplet loss to the problem of entity resolution. However, the use of word vector embeddings to genereate distributed representations of records has outperformed the state of the art ~\cite{ebraheem2017deeper}.

\item Hybrid human-machine workflows that combine crowdsourcing and active learning ~\cite{wang2012crowder,gokhale2014corleone} as well as tree-based ensembles for supervised learning ~\cite{varma2017relic} are alternate strands of research towards more efficient and more reliable entity matching.

\end{itemize}


\section{Define Success}
\begin{quote}
\emph{When / How do you know if you have succeeded in this project?
In other words, what is the minimum finding that would make this project a success and publishable?}
\end{quote}

Developing a model that can exceed the entity resolution accuracy set by state-of-the-art machine learning and crowd approaches would make this project a success and publishable, particularly if our model shows high accuracy when a training dataset is high in noise and have unstructured attributes.

\bibliographystyle{ieeetr}
\bibliography{prospectus}

\end{document}
