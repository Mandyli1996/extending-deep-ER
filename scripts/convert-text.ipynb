{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:26:55.630549Z",
     "start_time": "2018-03-03T22:26:55.622275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting convert-text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert-text.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse as ap\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "parser = ap.ArgumentParser()\n",
    "parser.add_argument('source_dir',\n",
    "                    help='directory containing dataset and match files to split')\n",
    "parser.add_argument('dest_dir',\n",
    "                    help='directory to save split dataset csvs')\n",
    "parser.add_argument('mapping_file',\n",
    "                    help='double dictionary containing maps to-from words\\\n",
    "                          and vocabulary indices')\n",
    "parser.add_argument('--set1', '-s1', default='set1.csv',\n",
    "                    help='filename of first dataset csv')\n",
    "parser.add_argument('--set2', '-s2', default='set2.csv',\n",
    "                    help='filename of second dataset csv')\n",
    "parser.add_argument('--matches', '-m', default='matches.csv',\n",
    "                    help='filename of positives matches csv')\n",
    "parser.add_argument('--indices', '-i', nargs='+', type=int,\n",
    "                    help='indices of columns to be converted (starting from 0)')\n",
    "parser.add_argument('--verbose', '-v', action='store_true',\n",
    "                    help='print statistics')\n",
    "\n",
    "# parse command line arguments\n",
    "args = parser.parse_args()\n",
    "source_dir = args.source_dir\n",
    "dest_dir = args.dest_dir\n",
    "mapping_file = args.mapping_file\n",
    "column_idxs = args.indices\n",
    "\n",
    "verbose = args.verbose\n",
    "\n",
    "set1 = args.set1\n",
    "set2 = args.set2\n",
    "matches = args.matches\n",
    "\n",
    "if verbose:\n",
    "    print('Loading datasets and maps.')\n",
    "# load data\n",
    "# df_pos is loaded so that it can be copied to destination directory\n",
    "df1 = pd.read_csv(os.path.join(source_dir, set1), encoding = \"latin1\")\n",
    "df2 = pd.read_csv(os.path.join(source_dir, set2), encoding = \"latin1\")\n",
    "df_pos = pd.read_csv(os.path.join(source_dir, matches), encoding = \"latin1\")\n",
    "\n",
    "# make column names the same\n",
    "assert(df1.columns[0] == 'id1')\n",
    "assert(df2.columns[0] == 'id2')\n",
    "df2.columns = [df2.columns[0]] + list(df1.columns[1:])\n",
    "\n",
    "# load double dictionary\n",
    "with open(mapping_file, 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "\n",
    "def clean_text(x):\n",
    "    \"formats a single string\"\n",
    "    if not isinstance(x, str):\n",
    "        return 'NaN'\n",
    "    \n",
    "    # separate possessives with spaces\n",
    "    x = x.replace('\\'s', ' \\'s')\n",
    "    \n",
    "    # convert html escape characters to regular characters\n",
    "    x = html.unescape(x)\n",
    "    \n",
    "    # separate punctuations with spaces\n",
    "    def pad(x):\n",
    "        match = re.findall(r'.', x[0])[0]\n",
    "        match_clean = ' ' + match + ' '\n",
    "        return match_clean\n",
    "    rx = r'\\(|\\)|/|!|#|\\$|%|&|\\\\|\\*|\\+|,|:|;|<|=|>|\\?|@|\\[|\\]|\\^|_|{|}|\\||'\n",
    "    rx += r'`|~'\n",
    "    x = re.sub(rx, pad, x)\n",
    "    \n",
    "    # remove decimal parts of version numbers\n",
    "    def v_int(x):\n",
    "        return re.sub('\\.\\d+','',x[0])\n",
    "    x = re.sub(r'v\\d+\\.\\d+', v_int, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "if verbose:\n",
    "    print('Cleaning the following columns from set1:')\n",
    "    for column in df1.columns[column_idxs]:\n",
    "        print(column, end=' ')\n",
    "    print()\n",
    "    print('Cleaning the following columns from set2:')\n",
    "    for column in df2.columns[column_idxs]:\n",
    "        print(column, end=' ')\n",
    "    print()\n",
    "\n",
    "df1.iloc[:, column_idxs] = df1.iloc[:, column_idxs].applymap(clean_text)\n",
    "df2.iloc[:, column_idxs] = df2.iloc[:, column_idxs].applymap(clean_text)\n",
    "\n",
    "def record2idx(x):\n",
    "    x = x.split()\n",
    "    for i, token in enumerate(x):\n",
    "        idx = map['word2idx'][token]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][token.lower()]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][string.capwords(token)]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][token.upper()]\n",
    "        x[i] = idx\n",
    "    return x\n",
    "\n",
    "if verbose:\n",
    "    print('Converting tokens to indices.')\n",
    "df1.iloc[:, column_idxs] = df1.iloc[:, column_idxs].applymap(record2idx)\n",
    "df2.iloc[:, column_idxs] = df2.iloc[:, column_idxs].applymap(record2idx)\n",
    "\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "    if verbose:\n",
    "        print('Creating destination directory')\n",
    "    \n",
    "df1.to_csv(os.path.join(dest_dir, set1), index=False)\n",
    "df2.to_csv(os.path.join(dest_dir, set2), index=False)\n",
    "df_pos.to_csv(os.path.join(dest_dir, matches), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:24:24.442682Z",
     "start_time": "2018-03-03T22:24:24.412697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse as ap\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# parse command line arguments\n",
    "source_dir = '../data/raw/amazon-google'\n",
    "dest_dir = '../data/converted/amazon-google'\n",
    "mapping_file = '../data/embeddings/glove-300.map'\n",
    "column_idxs = [3,4,5,7,8,9,10]\n",
    "\n",
    "verbose = True\n",
    "\n",
    "set1 = 'set1.csv'\n",
    "set2 = 'set2.csv'\n",
    "matches = 'matches.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:24:28.870935Z",
     "start_time": "2018-03-03T22:24:27.586166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets and maps.\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    print('Loading datasets and maps.')\n",
    "# load data\n",
    "# df_pos is loaded so that it can be copied to destination directory\n",
    "df1 = pd.read_csv(os.path.join(source_dir, set1), encoding = \"latin1\")\n",
    "df2 = pd.read_csv(os.path.join(source_dir, set2), encoding = \"latin1\")\n",
    "df_pos = pd.read_csv(os.path.join(source_dir, matches), encoding = \"latin1\")\n",
    "\n",
    "# load double dictionary\n",
    "with open(mapping_file, 'rb') as f:\n",
    "    map = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:53:13.314893Z",
     "start_time": "2018-03-03T22:53:13.294274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nan'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map['word2idx']['NAN']\n",
    "map['idx2word'][33248]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:25:25.221939Z",
     "start_time": "2018-03-03T22:25:25.217465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id1', 'title', 'description', 'manufacturer', 'price'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T22:26:36.187357Z",
     "start_time": "2018-03-03T22:26:36.182645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id2', 'title', 'description', 'manufacturer', 'price']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df2.columns[0]] + list(df1.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T08:26:18.145543Z",
     "start_time": "2018-02-25T08:26:17.052016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the following columns from set1:\n",
      "brand pcategory1 title techdetails proddescrshort proddescrlong imageurl \n",
      "Cleaning the following columns from set2:\n",
      "brand groupname title shelfdescr shortdescr longdescr imageurl \n"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    \"formats a single string\"\n",
    "    if not isinstance(x, str):\n",
    "        return 'NaN'\n",
    "    \n",
    "    # separate possessives with spaces\n",
    "    x = x.replace('\\'s', ' \\'s')\n",
    "    \n",
    "    # convert html escape characters to regular characters\n",
    "    x = html.unescape(x)\n",
    "    \n",
    "    # separate punctuations with spaces\n",
    "    def pad(x):\n",
    "        match = re.findall(r'.', x[0])[0]\n",
    "        match_clean = ' ' + match + ' '\n",
    "        return match_clean\n",
    "    rx = r'\\(|\\)|/|!|#|\\$|%|&|\\\\|\\*|\\+|,|:|;|<|=|>|\\?|@|\\[|\\]|\\^|_|{|}|\\||'\n",
    "    rx += r'`|~'\n",
    "    x = re.sub(rx, pad, x)\n",
    "    \n",
    "    # remove decimal parts of version numbers\n",
    "    def v_int(x):\n",
    "        return re.sub('\\.\\d+','',x[0])\n",
    "    x = re.sub(r'v\\d+\\.\\d+', v_int, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "if verbose:\n",
    "    print('Cleaning the following columns from set1:')\n",
    "    for column in df1.columns[column_idxs]:\n",
    "        print(column, end=' ')\n",
    "    print()\n",
    "    print('Cleaning the following columns from set2:')\n",
    "    for column in df2.columns[column_idxs]:\n",
    "        print(column, end=' ')\n",
    "    print()\n",
    "\n",
    "df1.iloc[:, column_idxs] = df1.iloc[:, column_idxs].applymap(clean_text)\n",
    "df2.iloc[:, column_idxs] = df2.iloc[:, column_idxs].applymap(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def record2idx(x):\n",
    "    x = x.split()\n",
    "    for i, token in enumerate(x):\n",
    "        idx = map['word2idx'][token]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][token.lower()]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][string.capwords(token)]\n",
    "        if idx == 0:\n",
    "            idx = map['word2idx'][token.upper()]\n",
    "        x[i] = idx\n",
    "    return x\n",
    "\n",
    "if verbose:\n",
    "    print('Converting tokens to indices.')\n",
    "df1.iloc[:, column_idxs] = df1.iloc[:, column_idxs].applymap(record2idx)\n",
    "df2.iloc[:, column_idxs] = df2.iloc[:, column_idxs].applymap(record2idx)\n",
    "\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "    if verbose:\n",
    "        print('Creating destination directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-25T05:02:10.322585Z",
     "start_time": "2018-02-25T05:02:10.305625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting convert-text.py\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "df1.to_csv(os.path.join(dest_dir, set1), index=False)\n",
    "df2.to_csv(os.path.join(dest_dir, set2), index=False)\n",
    "df_pos.to_csv(os.path.join(dest_dir, matches), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
