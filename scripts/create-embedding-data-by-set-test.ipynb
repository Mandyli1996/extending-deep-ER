{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-Google: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:49:02.302227Z",
     "start_time": "2018-03-26T19:48:48.086946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\tname\n",
      "\tdescription\n",
      "Cleaning text. Fixed 402 unique tokens\n",
      "Creating map file. 17263 unique tokens detected.\n",
      "Building embedding matrix. 2309 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/amazon-google ../data/converted/amazon-google-reduced -c name description -s --max_df=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-Google: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T03:34:01.494605Z",
     "start_time": "2018-04-02T03:33:45.117638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\tname\n",
      "\tdescription\n",
      "Cleaning text. Fixed 402 unique tokens\n",
      "Creating map file. 22339 unique tokens detected.\n",
      "Building embedding matrix. 5110 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/amazon-google ../data/converted/amazon-google -c name description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-walmart: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:36:24.444522Z",
     "start_time": "2018-04-12T21:35:46.825633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\tbrand\n",
      "\tgroupname\n",
      "\ttitle\n",
      "\tshelfdescr\n",
      "\tshortdescr\n",
      "\tlongdescr\n",
      "Columns to drop:\n",
      "\tprod_id1\n",
      "\tprod_id2\n",
      "\timageurl\n",
      "\tmodelno\n",
      "Cleaning text. Fixed 3719 unique tokens\n",
      "Creating map file. 69244 unique tokens detected.\n",
      "Building embedding matrix. 33265 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/amazon-walmart ../data/converted/amazon-walmart-reduced -c brand groupname title shelfdescr shortdescr longdescr -d prod_id1 prod_id2 imageurl modelno -s --max_df=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-Walmart: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:11:25.480137Z",
     "start_time": "2018-04-12T21:11:00.755443Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\tbrand\n",
      "\tgroupname\n",
      "\ttitle\n",
      "\tshelfdescr\n",
      "\tshortdescr\n",
      "\tlongdescr\n",
      "Columns to drop:\n",
      "\tprod_id1\n",
      "\tprod_id2\n",
      "\timageurl\n",
      "\tmodelno\n",
      "Cleaning text. Fixed 3719 unique tokens\n",
      "Creating map file. 124127 unique tokens detected.\n",
      "Building embedding matrix. 53741 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/amazon-walmart ../data/converted/amazon-walmart -c brand groupname title shelfdescr shortdescr longdescr -d prod_id1 prod_id2 imageurl modelno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBLP-Scholar: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T20:07:57.588938Z",
     "start_time": "2018-04-15T20:07:36.520272Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\ttitle\n",
      "\tauthors\n",
      "\tvenue\n",
      "Cleaning text. Fixed 4128 unique tokens\n",
      "Creating map file. 92475 unique tokens detected.\n",
      "Building embedding matrix. 45412 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/dblp-scholar ../data/converted/dblp-scholar-reduced -c title authors venue -s --max_df=1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBLP-Scholar: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T20:11:23.928507Z",
     "start_time": "2018-04-15T20:11:06.122565Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim model...\n",
      "Check id column names are valid: passed\n",
      "Check datasets have same column names: passed\n",
      "Check listed columns are valid: passed\n",
      "Columns to convert:\n",
      "\ttitle\n",
      "\tauthors\n",
      "\tvenue\n",
      "Cleaning text. Fixed 4128 unique tokens\n",
      "Creating map file. 123171 unique tokens detected.\n",
      "Building embedding matrix. 37557 unknown tokens assigned random Gaussian.\n",
      "Converting text data to index vectors.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python create-embedding-data-by-set.py glove ../data/embeddings/glove-300.gensim ../data/raw/dblp-scholar ../data/converted/dblp-scholar -c title authors venue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:35:31.928724Z",
     "start_time": "2018-04-12T21:35:31.920355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create-embedding-data-by-set.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create-embedding-data-by-set.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import argparse as ap\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# parse command-line arguments\n",
    "parser = ap.ArgumentParser()\n",
    "parser.add_argument('embedding_type',\n",
    "                    help = 'choose embedding type (word2vec or glove)')\n",
    "parser.add_argument('embedding_file',\n",
    "                    help = 'file path of downloaded embedding data')\n",
    "parser.add_argument('data_dir',\n",
    "                    help = 'directory containing dataset and match files')\n",
    "parser.add_argument('dest_dir',\n",
    "                    help = 'directory path to generate new files')\n",
    "parser.add_argument('--columns', '-c', nargs='+', required=True,\n",
    "                    help = 'names of columns to be converted')\n",
    "parser.add_argument('--drop', '-d', nargs='+', required=False,\n",
    "                    help = 'names of columns to be dropped')\n",
    "parser.add_argument('--set1', '-s1', default='set1.csv',\n",
    "                    help='filename of first dataset csv')\n",
    "parser.add_argument('--set2', '-s2', default='set2.csv',\n",
    "                    help='filename of second dataset csv')\n",
    "parser.add_argument('--matches', '-m', default='matches.csv',\n",
    "                    help='filename of positives matches csv')\n",
    "parser.add_argument('--sklearn', '-s', action='store_true',\n",
    "                    help='whether to use sklearn\\'s CountVectorizer')\n",
    "parser.add_argument('--max_df', default=0.1, type=float,\n",
    "                    help='proportion of documents above which token will be excluded')\n",
    "\n",
    "args = parser.parse_args()\n",
    "embedding_type = args.embedding_type\n",
    "embedding_file = args.embedding_file\n",
    "dest_dir = args.dest_dir\n",
    "data_dir = args.data_dir\n",
    "set1 = args.set1\n",
    "set2 = args.set2\n",
    "matches = args.matches\n",
    "columns = args.columns\n",
    "drop = args.drop\n",
    "use_sklearn = args.sklearn\n",
    "max_df = args.max_df\n",
    "\n",
    "print('Loading Gensim model...')\n",
    "if embedding_type in ['glove', 'word2vec']:\n",
    "    model = KeyedVectors.load(embedding_file)      \n",
    "elif embedding_type == 'debug':\n",
    "    print(embedding_type)\n",
    "    print(embedding_file)\n",
    "    print(dest_dir)\n",
    "    quit()\n",
    "else:\n",
    "    raise 'Not a valid embedding type'\n",
    "gensim_vocab = defaultdict(int, model.vocab)\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(data_dir, set1), encoding = \"latin1\")\n",
    "df2 = pd.read_csv(os.path.join(data_dir, set2), encoding = \"latin1\")\n",
    "\n",
    "# check input data meets requirements\n",
    "df1_column_check = list(df1.columns)\n",
    "df2_column_check = list(df2.columns)\n",
    "\n",
    "print('Check id column names are valid: ', end='')\n",
    "assert('id1' in df1_column_check)\n",
    "assert('id2' in df2_column_check)\n",
    "print('passed')\n",
    "\n",
    "df1_column_check.remove('id1')\n",
    "df2_column_check.remove('id2')\n",
    "\n",
    "print('Check datasets have same column names: ', end='')\n",
    "assert(df1_column_check == df2_column_check)\n",
    "print('passed')\n",
    "\n",
    "print('Check listed columns are valid: ', end='')\n",
    "for column in columns:\n",
    "    assert(column in df1_column_check)\n",
    "if drop:\n",
    "    for column in drop:\n",
    "        assert(column in df1_column_check)\n",
    "print('passed')\n",
    "\n",
    "print('Columns to convert:')\n",
    "for column in columns:\n",
    "    print('\\t' + column)\n",
    "\n",
    "if drop:\n",
    "    print('Columns to drop:')\n",
    "    for column in drop:\n",
    "        print('\\t' + column)\n",
    "        df1 = df1.drop(column, axis='columns')\n",
    "        df2 = df2.drop(column, axis='columns')\n",
    "\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "\n",
    "# no need to do anything to matches.csv, so just copy it to destination\n",
    "matches_source_path = os.path.join(data_dir, matches)\n",
    "matches_dest_path = os.path.join(dest_dir, matches)\n",
    "shutil.copyfile(matches_source_path, matches_dest_path)\n",
    "\n",
    "# pre-process all text data into tokens compatible with Glove/Word2Vec\n",
    "def clean_text(x):\n",
    "    \"formats a single string\"\n",
    "    if not isinstance(x, str):\n",
    "        return 'NaN'\n",
    "    \n",
    "    # separate possessives with spaces\n",
    "    x = x.replace('\\'s', ' \\'s')\n",
    "    \n",
    "    # convert html escape characters to regular characters\n",
    "    x = html.unescape(x)\n",
    "    \n",
    "    # separate punctuations with spaces\n",
    "    def pad(x):\n",
    "        match = re.findall(r'.', x[0])[0]\n",
    "        match_clean = ' ' + match + ' '\n",
    "        return match_clean\n",
    "    rx = r'\\(|\\)|/|!|#|\\$|%|&|\\\\|\\*|\\+|,|:|;|<|=|>|\\?|@|\\[|\\]|\\^|_|{|}|\\||'\n",
    "    rx += r'`|~'\n",
    "    x = re.sub(rx, pad, x)\n",
    "    \n",
    "    # remove decimal parts of version numbers\n",
    "    def v_int(x):\n",
    "        return re.sub('\\.\\d+','',x[0])\n",
    "    x = re.sub(r'v\\d+\\.\\d+', v_int, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "print('Cleaning text. ', end = '')\n",
    "df1.loc[:, columns] = df1.loc[:, columns].applymap(clean_text)\n",
    "df2.loc[:, columns] = df2.loc[:, columns].applymap(clean_text)\n",
    "\n",
    "# for any tokens not in model vocabulary, try a few capitalization variants\n",
    "fixed = list()\n",
    "def check_tokens(x):\n",
    "    global fixed\n",
    "    x = x.split()\n",
    "    new_string = ''\n",
    "    for token_orig in x:\n",
    "        token = token_orig\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = token.lower()\n",
    "            if bool(gensim_vocab[token]):\n",
    "                fixed.append(token_orig)\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = string.capwords(token)\n",
    "            if bool(gensim_vocab[token]):\n",
    "                fixed.append(token_orig)\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = token.upper()\n",
    "            if bool(gensim_vocab[token]):\n",
    "                fixed.append(token_orig)\n",
    "        new_string = new_string + ' ' + token\n",
    "    return new_string\n",
    "df1.loc[:, columns] = df1.loc[:, columns].applymap(check_tokens)\n",
    "df2.loc[:, columns] = df2.loc[:, columns].applymap(check_tokens)\n",
    "print('Fixed {} unique tokens'.format(pd.Series(fixed).nunique()))\n",
    "        \n",
    "# map each token to an index and convert text fields accordingly\n",
    "\n",
    "print('Creating map file. ', end='')\n",
    "# collapse all text columns in both datasets to a single list of strings\n",
    "corpus = list()\n",
    "for df in [df1, df2]:\n",
    "    for column in columns:\n",
    "        corpus.extend(list(df[column]))\n",
    "\n",
    "# map each token to a unique non-zero index\n",
    "word2idx = defaultdict(int)\n",
    "idx2word = defaultdict(str)\n",
    "if not use_sklearn:\n",
    "    i = 1\n",
    "    # missing = ['nan', 'NAN', 'Nan', 'NaN']\n",
    "    for instance in corpus:\n",
    "        for token in instance.split():\n",
    "            if not word2idx[token]:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "    # create a reverse mapping from index to token\n",
    "    for key, value in word2idx.items():\n",
    "        idx2word[value] = key\n",
    "else:\n",
    "    cv = CountVectorizer(max_df=max_df)\n",
    "    cv.fit(corpus)\n",
    "    sk_dict = cv.vocabulary_\n",
    "    # offset indices assigned by sklearn so 0 index is free\n",
    "    for word, index in sk_dict.items():\n",
    "        word2idx[word] = index + 1\n",
    "        idx2word[index+1] = word\n",
    "    \n",
    "print('{} unique tokens detected.'.format(len(word2idx)))\n",
    "    \n",
    "print('Building embedding matrix. ', end='')\n",
    "# an extra row of zeros at top of matrix is needed for Keras zero padding\n",
    "embedding_matrix = np.zeros([len(word2idx) + 1, 300])\n",
    "n_unknowns = 0\n",
    "for word, index in word2idx.items():\n",
    "    # if word has no vector embedding, leave corresponding row to be an\n",
    "    # ...attenuated random Gaussian\n",
    "    if bool(gensim_vocab[word]):\n",
    "        embedding_vector = model.get_vector(word)      \n",
    "    else:\n",
    "        embedding_vector = np.random.randn(300) / 300\n",
    "        n_unknowns += 1\n",
    "    embedding_matrix[index, :] = embedding_vector\n",
    "print('{} unknown tokens assigned random Gaussian.'.format(n_unknowns))\n",
    "    \n",
    "print('Converting text data to index vectors.')\n",
    "def record2idx(x):\n",
    "    if not use_sklearn:\n",
    "        x = x.split()\n",
    "    else:\n",
    "        x = x.lower().split()\n",
    "    return [word2idx[word] for word in x]\n",
    "\n",
    "df1.loc[:, columns] = df1.loc[:, columns].applymap(record2idx)\n",
    "df2.loc[:, columns] = df2.loc[:, columns].applymap(record2idx)\n",
    "\n",
    "# save files\n",
    "df1.to_csv(os.path.join(dest_dir, set1), index=False)\n",
    "df2.to_csv(os.path.join(dest_dir, set2), index=False)\n",
    "np.save(arr=embedding_matrix,\n",
    "        file=os.path.join(dest_dir, '{}-300.matrix'.format(embedding_type)))\n",
    "\n",
    "# save both word2idx and idx2word mappings into a double dictionary\n",
    "map = dict(word2idx = word2idx, idx2word = idx2word)\n",
    "with open(os.path.join(dest_dir, embedding_type + '-300.map'), 'wb') as f:\n",
    "    pkl.dump(map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:24:58.207885Z",
     "start_time": "2018-03-26T18:24:56.622121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:26:27.825686Z",
     "start_time": "2018-03-26T18:26:27.804610Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    \"formats a single string\"\n",
    "    if not isinstance(x, str):\n",
    "        return 'NaN'\n",
    "    \n",
    "    # separate possessives with spaces\n",
    "    x = x.replace('\\'s', ' \\'s')\n",
    "    \n",
    "    # convert html escape characters to regular characters\n",
    "    x = html.unescape(x)\n",
    "    \n",
    "    # separate punctuations with spaces\n",
    "    def pad(x):\n",
    "        match = re.findall(r'.', x[0])[0]\n",
    "        match_clean = ' ' + match + ' '\n",
    "        return match_clean\n",
    "    rx = r'\\(|\\)|/|!|#|\\$|%|&|\\\\|\\*|\\+|,|:|;|<|=|>|\\?|@|\\[|\\]|\\^|_|{|}|\\||'\n",
    "    rx += r'`|~'\n",
    "    x = re.sub(rx, pad, x)\n",
    "    \n",
    "    # remove decimal parts of version numbers\n",
    "    def v_int(x):\n",
    "        return re.sub('\\.\\d+','',x[0])\n",
    "    x = re.sub(r'v\\d+\\.\\d+', v_int, x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:25:14.630053Z",
     "start_time": "2018-03-26T18:25:05.551642Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load('../data/embeddings/glove-300.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:26:32.876960Z",
     "start_time": "2018-03-26T18:26:32.755339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/raw/amazon-google/set1.csv', encoding='latin1')\n",
    "df2 = pd.read_csv('../data/raw/amazon-google/set2.csv', encoding='latin1')\n",
    "corpus = list(df1['name']) + list(df1['description']) + list(df2['name']) + list(df2['description'])  \n",
    "corpus = [clean_text(x) for x in corpus]\n",
    "\n",
    "for index, record in enumerate(corpus):\n",
    "    if not isinstance(corpus[index], str):\n",
    "        corpus[index] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:36:40.928331Z",
     "start_time": "2018-03-26T18:36:40.608279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.05,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.05)\n",
    "cv.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:36:41.108830Z",
     "start_time": "2018-03-26T18:36:41.102999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adobe': 0,\n",
       " 'all': 1,\n",
       " 'an': 2,\n",
       " 'and': 3,\n",
       " 'are': 4,\n",
       " 'as': 5,\n",
       " 'by': 6,\n",
       " 'can': 7,\n",
       " 'cd': 8,\n",
       " 'complete': 9,\n",
       " 'create': 10,\n",
       " 'dvd': 11,\n",
       " 'easy': 12,\n",
       " 'edition': 13,\n",
       " 'encore': 14,\n",
       " 'features': 15,\n",
       " 'for': 16,\n",
       " 'from': 17,\n",
       " 'home': 18,\n",
       " 'in': 19,\n",
       " 'is': 20,\n",
       " 'it': 21,\n",
       " 'mac': 22,\n",
       " 'microsoft': 23,\n",
       " 'more': 24,\n",
       " 'new': 25,\n",
       " 'of': 26,\n",
       " 'on': 27,\n",
       " 'or': 28,\n",
       " 'pc': 29,\n",
       " 'pro': 30,\n",
       " 'professional': 31,\n",
       " 'software': 32,\n",
       " 'system': 33,\n",
       " 'that': 34,\n",
       " 'the': 35,\n",
       " 'this': 36,\n",
       " 'time': 37,\n",
       " 'to': 38,\n",
       " 'tools': 39,\n",
       " 'use': 40,\n",
       " 'user': 41,\n",
       " 'will': 42,\n",
       " 'win': 43,\n",
       " 'windows': 44,\n",
       " 'with': 45,\n",
       " 'xp': 46,\n",
       " 'you': 47,\n",
       " 'your': 48}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T18:31:39.300064Z",
     "start_time": "2018-03-26T18:31:39.295866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8356000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus) * 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T23:25:39.136747Z",
     "start_time": "2018-03-18T23:25:38.987759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = defaultdict(int)\n",
    "i = 1\n",
    "missing = ['NaN','nan', 'NAN', 'Nan']\n",
    "for sentence in corpus:\n",
    "    for token in sentence.split():\n",
    "        if token not in missing and not word2idx[token]:\n",
    "            word2idx[token] = i\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T01:20:06.553660Z",
     "start_time": "2018-03-19T01:20:06.453832Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/converted/amazon-google/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "    \n",
    "embedding_matrix = np.load('../data/converted/amazon-google/glove-300.matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T01:20:09.906239Z",
     "start_time": "2018-03-19T01:20:09.892644Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 8\n",
    "np.all(embedding_matrix[test_idx,:] == model.get_vector(map['idx2word'][test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T00:31:10.856474Z",
     "start_time": "2018-03-19T00:31:10.843382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_tokens(x):\n",
    "    x = x.split()\n",
    "    new_string = ''\n",
    "    for token_orig in x:\n",
    "        token = token_orig\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = token.lower()\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = string.capwords(token)\n",
    "        if not bool(gensim_vocab[token]):\n",
    "            token = token.upper()\n",
    "        new_string = new_string + ' ' + token\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T00:31:12.982292Z",
     "start_time": "2018-03-19T00:31:12.614032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLICKART 950 000 - premier image pack ( dvd-r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>broderbund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca international - arcserve lap / desktop oem...</td>\n",
       "      <td>oem arcserve backup v11 win 30u for laptops a...</td>\n",
       "      <td>computer associates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noah 's ark activity center ( jewel case ages...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>victory multimedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peachtree by sage premium accounting for nonp...</td>\n",
       "      <td>peachtree premium accounting for nonprofits 2...</td>\n",
       "      <td>sage software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>singing coach unlimited</td>\n",
       "      <td>singing coach unlimited - electronic learning...</td>\n",
       "      <td>CARRY-A-TUNE technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emc retrospect 7.5 disk to disk windows</td>\n",
       "      <td>emc retrospect 7.5 disk to DISKCROMWINDOWS</td>\n",
       "      <td>Dantz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adobe after effects professional 6.5 upgrade ...</td>\n",
       "      <td>upgrade only ; installation of after effects ...</td>\n",
       "      <td>adobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>acad upgrade dragon naturallyspeaking pro sol...</td>\n",
       "      <td>- marketing information : dragon NATURALLYSPE...</td>\n",
       "      <td>nuance academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mia 's math adventure : just in time</td>\n",
       "      <td>in mia 's math adventure : just in time child...</td>\n",
       "      <td>Kutoka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>disney 's 1st &amp; 2nd grade bundle ( pixar 1st ...</td>\n",
       "      <td>disney 's 1st &amp; 2nd grade bundle will help yo...</td>\n",
       "      <td>disney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>paper art : gift wrapping</td>\n",
       "      <td>how many times have you heard that it 's the ...</td>\n",
       "      <td>arc media inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nfs maestro solo 2006 1u</td>\n",
       "      <td>- marketing information : nfs maestro solo is...</td>\n",
       "      <td>hummingbird communications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>microsoft sql server standard edition 2005 64...</td>\n",
       "      <td>sql server is a comprehensive integrated end-...</td>\n",
       "      <td>microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spy sweeper spanish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>webroot software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>domino designer / developer v5</td>\n",
       "      <td>reference domino designer / developer r5 doc ...</td>\n",
       "      <td>lotus development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OMNIOUTLINER professional 3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compaq comp. secure path V3C-NETWARE WRKGRP 5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>compaq computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>upg SGMS 1000 incremental node</td>\n",
       "      <td>today enterprises and service providers face ...</td>\n",
       "      <td>sonic systems inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the human body</td>\n",
       "      <td>in topics presents the human body you'll unco...</td>\n",
       "      <td>topics entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>school zone PENCIL-PAL software big phonics (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>school zone interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>millie 's math house ages 3-7 ( win / mac )</td>\n",
       "      <td>now featuring addition subtraction and counti...</td>\n",
       "      <td>ibm ( aap misc parts )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>world book encyclopedia 2006</td>\n",
       "      <td>the world book encyclopedia 2006 is a truly s...</td>\n",
       "      <td>topics entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MUSICALIS guitar workshop</td>\n",
       "      <td>chord display / create your own chords / digi...</td>\n",
       "      <td>global software publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the printshop 20 professional publisher</td>\n",
       "      <td>the complete and easy publishing solution for...</td>\n",
       "      <td>broderbund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>data protection suite</td>\n",
       "      <td>if you use a computer you're at risk for syst...</td>\n",
       "      <td>stomp inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kids power fun for girls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>topics entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>adobe premiere pro cs3 upgrade</td>\n",
       "      <td>note : this is the upgrade version of adobe p...</td>\n",
       "      <td>adobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i copy dvds 2 professional edition</td>\n",
       "      <td>ICOPYDVDS2 pro edition contains all you need ...</td>\n",
       "      <td>me too software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gen-x media made easy vol. 3</td>\n",
       "      <td>media made easy vol. 3 is your front-row tick...</td>\n",
       "      <td>gen-x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>family tree maker 11.2 collector 's edition</td>\n",
       "      <td>whether you're an avid family historian or a ...</td>\n",
       "      <td>encore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>age of empires iii : the warchiefs expansion ...</td>\n",
       "      <td>age of empires iii : the war chiefs is the ne...</td>\n",
       "      <td>Destineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>adobe creative suite cs3 web standard</td>\n",
       "      <td>adobe creative suite 3 web standard software ...</td>\n",
       "      <td>adobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>guitar tool - worship edition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MIDISOFT corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>delorme topo usa mapping software 6.0 east re...</td>\n",
       "      <td>topo usa 6.0 eastern region makes it a snap t...</td>\n",
       "      <td>delorme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>nero 8 ultra edition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nero inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>weekly reader mastering high school 2008</td>\n",
       "      <td>the weekly reader mastering middle school lea...</td>\n",
       "      <td>FOGWARE publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>northstar for pc / mac</td>\n",
       "      <td>northstar ( win 98 me nt 2000 xp / mac 10.3.8...</td>\n",
       "      <td>true north technology inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>make music allegro 2007 LABPACK - 5 user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>makemusic !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>instant immersion hawaiian audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>topics entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>power translator 11 premium</td>\n",
       "      <td>power translator 11 premium is the world 's m...</td>\n",
       "      <td>language engineering co llc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>liquid ledger personal finance</td>\n",
       "      <td>personal finance software for max os x. liqui...</td>\n",
       "      <td>CSDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>sportsman 's double play</td>\n",
       "      <td>sportsman 's double play gives you the very b...</td>\n",
       "      <td>masque publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>jumpstart advanced preschool v2</td>\n",
       "      <td>jumpstart advanced preschool is the complete ...</td>\n",
       "      <td>knowledge adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>visual thesaurus the - Thinkmap inc.</td>\n",
       "      <td>Thinkmap 's visual thesaurus is a new type of...</td>\n",
       "      <td>CSDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>phonics success essentials : grades 7-9</td>\n",
       "      <td>this package contains learning in 6 different...</td>\n",
       "      <td>topics education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>COOK'N cd recipe cards</td>\n",
       "      <td>COOKN cd recipe cards ( win 95 98 me nt 2000 ...</td>\n",
       "      <td>dvo enterprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>adobe creative suite cs3 web standard upsell ...</td>\n",
       "      <td>note : this is the upsell version of adobe cr...</td>\n",
       "      <td>adobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>train sim modeler design studio</td>\n",
       "      <td>with train sim modeler you can create 3d TRAI...</td>\n",
       "      <td>abacus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>learn to play keyboard ( jewel case )</td>\n",
       "      <td>with learn to play keyboard you can avoid she...</td>\n",
       "      <td>alfred publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>ufo afterlight</td>\n",
       "      <td>ufo : afterlight mixes squad-based combat wit...</td>\n",
       "      <td>Topware interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>instant immersion italian deluxe v2 ( large b...</td>\n",
       "      <td>with instant immersion italian deluxe you'll ...</td>\n",
       "      <td>topics entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>enemy territory : quake wars</td>\n",
       "      <td>enemy territory : quake wars mac</td>\n",
       "      <td>aspyr media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>weekly reader mastering elementary / middle s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOGWARE publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>sentinel : descendants in time</td>\n",
       "      <td>sentinel : descendants in time gives you a ch...</td>\n",
       "      <td>dreamcatcher interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>photostory on cd &amp; dvd 5</td>\n",
       "      <td>magix photostory on cd and dvd 5 answers the ...</td>\n",
       "      <td>magix entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>flash remoting 1 alp ret eng cd 2u</td>\n",
       "      <td>- marketing information : macromedia flash re...</td>\n",
       "      <td>adobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>shapes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>school zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>dragon naturally speaking standard v9</td>\n",
       "      <td>dragon naturallyspeaking 9 ( standard edition...</td>\n",
       "      <td>nuance communications inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>MEDIARECOVER</td>\n",
       "      <td>MEDIARECOVER gives you the ability to recover...</td>\n",
       "      <td>aladdin systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>photo explosion 3.0</td>\n",
       "      <td>photo explosion 3.0</td>\n",
       "      <td>nova development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1363 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "0      CLICKART 950 000 - premier image pack ( dvd-r...   \n",
       "1      ca international - arcserve lap / desktop oem...   \n",
       "2      noah 's ark activity center ( jewel case ages...   \n",
       "3      peachtree by sage premium accounting for nonp...   \n",
       "4                               singing coach unlimited   \n",
       "5               emc retrospect 7.5 disk to disk windows   \n",
       "6      adobe after effects professional 6.5 upgrade ...   \n",
       "7      acad upgrade dragon naturallyspeaking pro sol...   \n",
       "8                  mia 's math adventure : just in time   \n",
       "9      disney 's 1st & 2nd grade bundle ( pixar 1st ...   \n",
       "10                            paper art : gift wrapping   \n",
       "11                             nfs maestro solo 2006 1u   \n",
       "12     microsoft sql server standard edition 2005 64...   \n",
       "13                                  spy sweeper spanish   \n",
       "14                       domino designer / developer v5   \n",
       "15                        OMNIOUTLINER professional 3.0   \n",
       "16     compaq comp. secure path V3C-NETWARE WRKGRP 5...   \n",
       "17                       upg SGMS 1000 incremental node   \n",
       "18                                       the human body   \n",
       "19     school zone PENCIL-PAL software big phonics (...   \n",
       "20          millie 's math house ages 3-7 ( win / mac )   \n",
       "21                         world book encyclopedia 2006   \n",
       "22                            MUSICALIS guitar workshop   \n",
       "23              the printshop 20 professional publisher   \n",
       "24                                data protection suite   \n",
       "25                             kids power fun for girls   \n",
       "26                       adobe premiere pro cs3 upgrade   \n",
       "27                   i copy dvds 2 professional edition   \n",
       "28                         gen-x media made easy vol. 3   \n",
       "29          family tree maker 11.2 collector 's edition   \n",
       "...                                                 ...   \n",
       "1333   age of empires iii : the warchiefs expansion ...   \n",
       "1334              adobe creative suite cs3 web standard   \n",
       "1335                      guitar tool - worship edition   \n",
       "1336   delorme topo usa mapping software 6.0 east re...   \n",
       "1337                               nero 8 ultra edition   \n",
       "1338           weekly reader mastering high school 2008   \n",
       "1339                             northstar for pc / mac   \n",
       "1340           make music allegro 2007 LABPACK - 5 user   \n",
       "1341                   instant immersion hawaiian audio   \n",
       "1342                        power translator 11 premium   \n",
       "1343                     liquid ledger personal finance   \n",
       "1344                           sportsman 's double play   \n",
       "1345                    jumpstart advanced preschool v2   \n",
       "1346               visual thesaurus the - Thinkmap inc.   \n",
       "1347            phonics success essentials : grades 7-9   \n",
       "1348                             COOK'N cd recipe cards   \n",
       "1349   adobe creative suite cs3 web standard upsell ...   \n",
       "1350                    train sim modeler design studio   \n",
       "1351              learn to play keyboard ( jewel case )   \n",
       "1352                                     ufo afterlight   \n",
       "1353   instant immersion italian deluxe v2 ( large b...   \n",
       "1354                       enemy territory : quake wars   \n",
       "1355   weekly reader mastering elementary / middle s...   \n",
       "1356                     sentinel : descendants in time   \n",
       "1357                           photostory on cd & dvd 5   \n",
       "1358                 flash remoting 1 alp ret eng cd 2u   \n",
       "1359                                             shapes   \n",
       "1360              dragon naturally speaking standard v9   \n",
       "1361                                       MEDIARECOVER   \n",
       "1362                                photo explosion 3.0   \n",
       "\n",
       "                                            description  \\\n",
       "0                                                   NaN   \n",
       "1      oem arcserve backup v11 win 30u for laptops a...   \n",
       "2                                                   NaN   \n",
       "3      peachtree premium accounting for nonprofits 2...   \n",
       "4      singing coach unlimited - electronic learning...   \n",
       "5            emc retrospect 7.5 disk to DISKCROMWINDOWS   \n",
       "6      upgrade only ; installation of after effects ...   \n",
       "7      - marketing information : dragon NATURALLYSPE...   \n",
       "8      in mia 's math adventure : just in time child...   \n",
       "9      disney 's 1st & 2nd grade bundle will help yo...   \n",
       "10     how many times have you heard that it 's the ...   \n",
       "11     - marketing information : nfs maestro solo is...   \n",
       "12     sql server is a comprehensive integrated end-...   \n",
       "13                                                  NaN   \n",
       "14     reference domino designer / developer r5 doc ...   \n",
       "15                                                  NaN   \n",
       "16                                                  NaN   \n",
       "17     today enterprises and service providers face ...   \n",
       "18     in topics presents the human body you'll unco...   \n",
       "19                                                  NaN   \n",
       "20     now featuring addition subtraction and counti...   \n",
       "21     the world book encyclopedia 2006 is a truly s...   \n",
       "22     chord display / create your own chords / digi...   \n",
       "23     the complete and easy publishing solution for...   \n",
       "24     if you use a computer you're at risk for syst...   \n",
       "25                                                  NaN   \n",
       "26     note : this is the upgrade version of adobe p...   \n",
       "27     ICOPYDVDS2 pro edition contains all you need ...   \n",
       "28     media made easy vol. 3 is your front-row tick...   \n",
       "29     whether you're an avid family historian or a ...   \n",
       "...                                                 ...   \n",
       "1333   age of empires iii : the war chiefs is the ne...   \n",
       "1334   adobe creative suite 3 web standard software ...   \n",
       "1335                                                NaN   \n",
       "1336   topo usa 6.0 eastern region makes it a snap t...   \n",
       "1337                                                NaN   \n",
       "1338   the weekly reader mastering middle school lea...   \n",
       "1339   northstar ( win 98 me nt 2000 xp / mac 10.3.8...   \n",
       "1340                                                NaN   \n",
       "1341                                                NaN   \n",
       "1342   power translator 11 premium is the world 's m...   \n",
       "1343   personal finance software for max os x. liqui...   \n",
       "1344   sportsman 's double play gives you the very b...   \n",
       "1345   jumpstart advanced preschool is the complete ...   \n",
       "1346   Thinkmap 's visual thesaurus is a new type of...   \n",
       "1347   this package contains learning in 6 different...   \n",
       "1348   COOKN cd recipe cards ( win 95 98 me nt 2000 ...   \n",
       "1349   note : this is the upsell version of adobe cr...   \n",
       "1350   with train sim modeler you can create 3d TRAI...   \n",
       "1351   with learn to play keyboard you can avoid she...   \n",
       "1352   ufo : afterlight mixes squad-based combat wit...   \n",
       "1353   with instant immersion italian deluxe you'll ...   \n",
       "1354                   enemy territory : quake wars mac   \n",
       "1355                                                NaN   \n",
       "1356   sentinel : descendants in time gives you a ch...   \n",
       "1357   magix photostory on cd and dvd 5 answers the ...   \n",
       "1358   - marketing information : macromedia flash re...   \n",
       "1359                                                NaN   \n",
       "1360   dragon naturallyspeaking 9 ( standard edition...   \n",
       "1361   MEDIARECOVER gives you the ability to recover...   \n",
       "1362                                photo explosion 3.0   \n",
       "\n",
       "                      manufacturer  \n",
       "0                       broderbund  \n",
       "1              computer associates  \n",
       "2               victory multimedia  \n",
       "3                    sage software  \n",
       "4        CARRY-A-TUNE technologies  \n",
       "5                            Dantz  \n",
       "6                            adobe  \n",
       "7                  nuance academic  \n",
       "8                           Kutoka  \n",
       "9                           disney  \n",
       "10                  arc media inc.  \n",
       "11      hummingbird communications  \n",
       "12                       microsoft  \n",
       "13                webroot software  \n",
       "14               lotus development  \n",
       "15                            CSDC  \n",
       "16                 compaq computer  \n",
       "17              sonic systems inc.  \n",
       "18            topics entertainment  \n",
       "19         school zone interactive  \n",
       "20          ibm ( aap misc parts )  \n",
       "21            topics entertainment  \n",
       "22      global software publishing  \n",
       "23                      broderbund  \n",
       "24                       stomp inc  \n",
       "25            topics entertainment  \n",
       "26                           adobe  \n",
       "27                 me too software  \n",
       "28                           gen-x  \n",
       "29                          encore  \n",
       "...                            ...  \n",
       "1333                     Destineer  \n",
       "1334                         adobe  \n",
       "1335          MIDISOFT corporation  \n",
       "1336                       delorme  \n",
       "1337                     nero inc.  \n",
       "1338            FOGWARE publishing  \n",
       "1339    true north technology inc.  \n",
       "1340                   makemusic !  \n",
       "1341          topics entertainment  \n",
       "1342   language engineering co llc  \n",
       "1343                          CSDC  \n",
       "1344             masque publishing  \n",
       "1345           knowledge adventure  \n",
       "1346                          CSDC  \n",
       "1347              topics education  \n",
       "1348               dvo enterprises  \n",
       "1349                         adobe  \n",
       "1350                        abacus  \n",
       "1351             alfred publishing  \n",
       "1352           Topware interactive  \n",
       "1353          topics entertainment  \n",
       "1354                   aspyr media  \n",
       "1355            FOGWARE publishing  \n",
       "1356      dreamcatcher interactive  \n",
       "1357           magix entertainment  \n",
       "1358                         adobe  \n",
       "1359                   school zone  \n",
       "1360    nuance communications inc.  \n",
       "1361               aladdin systems  \n",
       "1362              nova development  \n",
       "\n",
       "[1363 rows x 3 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[columns].applymap(clean_text).applymap(check_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:53:21.521504Z",
     "start_time": "2018-03-26T19:53:21.464339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('../data/converted/amazon-google-reduced/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "    \n",
    "embedding_matrix = np.load('../data/converted/amazon-google-reduced/glove-300.matrix.npy')\n",
    "\n",
    "word = 'school'\n",
    "idx = map['word2idx'][word]\n",
    "word_vector = embedding_matrix[idx,:]\n",
    "print(np.all(word_vector == model.get_vector(word)))\n",
    "print(word == map['idx2word'][idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
