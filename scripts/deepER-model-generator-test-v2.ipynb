{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:37:49.543436Z",
     "start_time": "2018-04-01T20:37:47.158000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,\\\n",
    "                            average_precision_score, roc_auc_score,\\\n",
    "                            roc_curve, precision_recall_curve, confusion_matrix,\\\n",
    "                            accuracy_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from matplotlib import rcParams\n",
    "from importlib import reload\n",
    "from model_generator import deep_er_model_generator\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = 'times new roman'\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:44:46.357755Z",
     "start_time": "2018-04-01T20:44:29.463871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(hp)\n",
    "\n",
    "with open('../data/converted/amazon-google/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "source_dir = os.path.join(data_dir,'split','amazon-google')\n",
    "data = hp.load_data(source_dir)\n",
    "\n",
    "datasets = ['train_1', 'val_1', 'test_1', 'train_2', 'val_2', 'test_2']\n",
    "\n",
    "data['train_2']['price'] = data['train_2']['price'].apply(hp.str_to_num)\n",
    "data['val_2']['price'] = data['val_2']['price'].apply(hp.str_to_num)\n",
    "data['test_2']['price'] = data['test_2']['price'].apply(hp.str_to_num)\n",
    "\n",
    "doc_freqs_1, doc_freqs_2 = hp.get_document_frequencies('../data/converted/amazon-google/', mapping=map)\n",
    "nan_idx = map['word2idx']['NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs to dense layer: 13\n"
     ]
    }
   ],
   "source": [
    "histories = dict(acc=list(), val_acc=list(), loss=list(), val_loss=list())\n",
    "model, X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "deep_er_model_generator(data,\n",
    "                        embedding_file = '../data/converted/amazon-google-reduced/glove-300.matrix.npy',\n",
    "                        text_columns = ['name', 'description'],\n",
    "                        numeric_columns = ['price'],\n",
    "                        text_nan_idx=nan_idx,\n",
    "                        num_nan_val=0,\n",
    "                        text_sim_metrics=['cosine','inverse_l1'],\n",
    "                        text_compositions=['average'],\n",
    "                        numeric_sim_metrics=['min_max_ratio', 'scaled_inverse_lp', 'unscaled_inverse_lp'],\n",
    "                        dense_nodes=[32, 16, 8],\n",
    "                        document_frequencies=(doc_freqs_1, doc_freqs_2),\n",
    "                        idf_smoothing=2,\n",
    "                        make_isna=True,\n",
    "                        embedding_trainable=True,\n",
    "                        batch_norm=True,\n",
    "                        dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:12.645116Z",
     "start_time": "2018-04-01T21:08:12.603115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T03:54:04.040360Z",
     "start_time": "2018-04-02T03:54:04.028321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_generator_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_generator_v2.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Lambda, Dot, Concatenate, \\\n",
    "                         Bidirectional, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def deep_er_model_generator_v2(data_dict,\n",
    "                            embedding_file,\n",
    "                            padding_limit = 100,\n",
    "                            mask_zero = True,\n",
    "                            embedding_trainable = False,\n",
    "                            text_columns = list(), \n",
    "                            numeric_columns = list(),\n",
    "                            make_isna = True,\n",
    "                            text_nan_idx = None,\n",
    "                            num_nan_val = None,\n",
    "                            text_compositions = ['average'],\n",
    "                            text_sim_metrics = ['cosine'],\n",
    "                            numeric_sim_metrics = ['unscaled_inverse_lp'],\n",
    "                            dense_nodes = [10],\n",
    "                            lstm_args = dict(units=50),\n",
    "                            document_frequencies = None,\n",
    "                            idf_smoothing = 2,\n",
    "                            batch_norm = False,\n",
    "                            dropout = 0):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of paired split DataFrames and returns a DeepER \n",
    "    model with data formatted for said model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        A dictionary of dataframes (pd.DataFrame) stored with the following\n",
    "        keys: train_1, val_1, test_1, train_2, val_2, test_2\n",
    "    embedding_file : str\n",
    "        The location and name of numpy matrix containing word vector\n",
    "        embeddings.\n",
    "    padding_limit : int, optional\n",
    "        The maximum length of any text sequence. For any text attribute whose\n",
    "        max length is below padding_limit, the max length will be used.\n",
    "        Otherwise, padding_limit will be used to both pad and truncuate\n",
    "        text sequences for that attribute.\n",
    "    mask_zero : bool, optional\n",
    "        Whether to ignore text sequence indices with value of 0. Useful for\n",
    "        LSTM's and variable length inputs.\n",
    "    embedding_trainable: bool, optional\n",
    "        Whether to allow the embedding layer to be fine tuned.\n",
    "    text_columns : list of strings, optional\n",
    "        A list of names of text-based attributes\n",
    "    numeric_columns : list of strings, optional\n",
    "        A list of names of numeric attributes\n",
    "    make_isna: bool, optional\n",
    "        Whether to create new attributes indicating the presence of null values\n",
    "        for each original attribute.\n",
    "    text_nan_idx : int, optional\n",
    "        The index corresponding to NaN values in text-based attributes.\n",
    "    num_nan_val : int, optional\n",
    "        The value corresponding to NaN values in numeric attributes.\n",
    "    text_compositions : list of strings, optional\n",
    "        List of composition methods to be applied to embedded text attributes.\n",
    "        Valid options are :\n",
    "            - average : a simple average of all embedded vectors\n",
    "            - idf : an average of all embedded vectors weighted by normalized\n",
    "                    inverse document frequency\n",
    "            - bi_lstm\n",
    "    text_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each text-based attribute.\n",
    "        Valid options are :\n",
    "            - cosine\n",
    "            - inverse_l1 : e^-[l1_distance]\n",
    "            - inverse_l2 : e^-[l2_distance]\n",
    "    numeric_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each numeric attribute.\n",
    "        Valid options are :\n",
    "            - scaled_inverse_lp : e^[-2(abs_diff)/sum]\n",
    "            - unscaled_inverse_lp : e^[-abs_diff]\n",
    "            - min_max_ratio : min / max\n",
    "    dense_nodes : list of ints, optional\n",
    "        Specifies topology of hidden dense layers\n",
    "    lstm_args = dict, optional\n",
    "        Keyword arguments for LSTM layer\n",
    "    document_frequencies = tuple of length 2, optional\n",
    "        Tuple of two lists of document frequencies, left side then right\n",
    "    idf_smoothing : int, optional\n",
    "        un-normalized idf = 1 / df ^ (1 / idf_smoothing)\n",
    "        Higher values means that high document frequency words are penalized\n",
    "        less.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### DATA PROCESSING ###\n",
    "    # initialize an empty dictionary for storing all data\n",
    "    # dictionary structure will be data[split][side][column]\n",
    "    sides = ['left', 'right']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = dict()\n",
    "    for split in splits:\n",
    "        data[split] = dict()\n",
    "        for side in sides:\n",
    "            data[split][side] = dict()\n",
    "            \n",
    "    columns = text_columns + numeric_columns\n",
    "    \n",
    "    # separate each feature into its own dictionary entry\n",
    "    for column in columns:\n",
    "        data['train']['left'][column] = data_dict['train_1'][column]\n",
    "        data['train']['right'][column] = data_dict['train_2'][column]\n",
    "\n",
    "        data['val']['left'][column] = data_dict['val_1'][column]\n",
    "        data['val']['right'][column] = data_dict['val_2'][column]\n",
    "\n",
    "        data['test']['left'][column] = data_dict['test_1'][column]\n",
    "        data['test']['right'][column] = data_dict['test_2'][column]\n",
    "    \n",
    "    # if enabled, create a binary column for each feature indicating whether\n",
    "    # it contains a missing value. for text data, this will be a list with\n",
    "    # a single index representing the 'NaN' token. for numeric data, this will\n",
    "    # likely be a 0.\n",
    "    if make_isna:\n",
    "        for split, side, column in it.product(splits, sides, text_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == [text_nan_idx])\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            isna_column = column + '_isna'\n",
    "            data[split][side][isna_column] = isna\n",
    "        for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == num_nan_val)\n",
    "            isna_column = column + '_isna'\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            data[split][side][isna_column] = isna\n",
    "    \n",
    "    # pad each text column according to the length of its longest entry in\n",
    "    # both datasets\n",
    "    maxlen = dict()\n",
    "    for column in text_columns:\n",
    "        maxlen_left = data['train']['left'][column].apply(lambda x: len(x)).max()\n",
    "        maxlen_right = data['train']['right'][column].apply(lambda x: len(x)).max()\n",
    "        maxlength = min(padding_limit, max(maxlen_left, maxlen_right))\n",
    "        for split, side in it.product(splits, sides):\n",
    "            data[split][side][column] = pad_sequences(data[split][side][column],\n",
    "                                                      maxlen=maxlength,\n",
    "                                                      padding='post',\n",
    "                                                      truncating='post')\n",
    "        maxlen[column] = maxlength\n",
    "    \n",
    "    # convert all numeric features to float and reshape to be 2-dimensional\n",
    "    for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "        feature = data[split][side][column]\n",
    "        feature = feature.values.astype(np.float32).reshape(-1,1)\n",
    "        data[split][side][column] = feature\n",
    "            \n",
    "    # format X values for each split as a list of 2-dimensional arrays\n",
    "    packaged_data = OrderedDict()\n",
    "    for split in splits:\n",
    "        packaged_data[split] = list()\n",
    "        for side, column in it.product(sides, columns):\n",
    "            packaged_data[split].append(data[split][side][column])\n",
    "        if make_isna:\n",
    "            for side, column in it.product(sides, columns):\n",
    "                packaged_data[split].append(data[split][side][column + '_isna'])\n",
    "    \n",
    "    # convert y-values\n",
    "    y_train = to_categorical(data_dict['train_y'])\n",
    "    y_val = to_categorical(data_dict['val_y'])\n",
    "    y_test = to_categorical(data_dict['test_y'])\n",
    "    \n",
    "    ### MODEL BUILDING ###\n",
    "    \n",
    "    # each attribute of each side is its own input tensor\n",
    "    # text input tensors for both sides are created before numeric input tensors\n",
    "    input_tensors = dict(left=dict(), right=dict())\n",
    "    for side, column in it.product(sides, text_columns):\n",
    "        input_tensors[side][column] = Input(shape=(maxlen[column],))\n",
    "        \n",
    "    for side, column in it.product(sides, numeric_columns):\n",
    "        input_tensors[side][column] = Input(shape=(1,))\n",
    "    \n",
    "    # create a single embedding layer for text input tensors\n",
    "    embedding_matrix = np.load(embedding_file)\n",
    "    embedding_layers = dict()\n",
    "    for composition in text_compositions:\n",
    "        embedding_layers[composition] = dict()\n",
    "        for similarity in text_sim_metrics:\n",
    "            embedding_layers[composition][similarity] = Embedding(embedding_matrix.shape[0],\n",
    "                                                        embedding_matrix.shape[1],\n",
    "                                                        weights=[embedding_matrix],\n",
    "                                                        trainable=embedding_trainable,\n",
    "                                                        mask_zero=mask_zero)\n",
    "            \n",
    "    \n",
    "    # use embedding_layer ot convert text input tensors to embedded tensors\n",
    "    # and store in dictionary.\n",
    "    # an embedding tensor will have shape n_words x n_embedding_dimensions\n",
    "    embedded_tensors = dict()\n",
    "    for composition in text_compositions:\n",
    "        embedded_tensors[composition] = dict()\n",
    "        for similarity in text_sim_metrics:\n",
    "            embedded_tensors[composition][similarity] = dict(left=dict(), right=dict())\n",
    "            for side, column in it.product(sides, text_columns):\n",
    "                embedded_tensors[composition][similarity][side][column] = \\\n",
    "                embedding_layers[composition][similarity](input_tensors[side][column])\n",
    "    \n",
    "    # initialize dictionary for storing a composition tensor for each embedding tensor\n",
    "    composed_tensors = dict()\n",
    "    for composition in text_compositions:\n",
    "        composed_tensors[composition] = dict()\n",
    "        for similarity in text_sim_metrics:\n",
    "            composed_tensors[composition][similarity] = dict()\n",
    "            for side in sides:\n",
    "                composed_tensors[composition][similarity][side] = dict()\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by averaging all embeddings\n",
    "    if 'average' in text_compositions:\n",
    "        averaging_layer = Lambda(lambda x: K.mean(x, axis=1), output_shape=(maxlen[column],))\n",
    "        for similarity in text_sim_metrics:\n",
    "            for side, column in it.product(sides, text_columns):\n",
    "                composed_tensors['average'][similarity][side][column] = \\\n",
    "                averaging_layer(embedded_tensors['average'][similarity][side][column])\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by taking a weighted average of all\n",
    "    # embeddings. \n",
    "    if 'idf' in text_compositions:\n",
    "        # store document frequency constants for each side\n",
    "        dfs_constant = dict()\n",
    "        dfs_constant['left'] = K.constant(document_frequencies[0])\n",
    "        dfs_constant['right'] = K.constant(document_frequencies[1])\n",
    "        \n",
    "        # a selection layer uses an input tensor as indices to select\n",
    "        # document frequencies from dfs_constant\n",
    "        dfs_selection_layer = dict()\n",
    "        \n",
    "        # a conversion layer converts a tensor of selected document frequencies\n",
    "        # to a tensor of inverse document frequencies. the larger the DF,\n",
    "        # the smaller the inverse, the smallness of which is controlled by\n",
    "        # idf_smoothing\n",
    "        idf_conversion_layer = Lambda(lambda x: 1 / (K.pow(x, 1/idf_smoothing)))\n",
    "        \n",
    "        # document frequencies of 0 will result in IDF's of inf. these should\n",
    "        # be converted back to 0's.\n",
    "        idf_fix_layer = Lambda(lambda x: tf.where(tf.is_inf(x), tf.zeros_like(x), x))\n",
    "        \n",
    "        # for each IDF tensor, scale its values so they sum to 1\n",
    "        idf_normalization_layer = Lambda(lambda x: x / K.expand_dims(K.sum(x, axis=1), axis=1))\n",
    "        \n",
    "        # take dot product between embedding tensor vectors and IDF weights\n",
    "        dot_layer = Dot(axes=1)\n",
    "        \n",
    "        for similarity in text_sim_metrics:\n",
    "            for side in sides:\n",
    "                dfs_selection_layer[side] = Lambda(lambda x: K.gather(dfs_constant[side], K.cast(x, tf.int32)))\n",
    "                for column in text_columns:                \n",
    "                    dfs_tensor = dfs_selection_layer[side](input_tensors[side][column])\n",
    "                    idfs_tensor = idf_conversion_layer(dfs_tensor)\n",
    "                    idfs_tensor_fixed = idf_fix_layer(idfs_tensor)\n",
    "                    idfs_tensor_normalized = idf_normalization_layer(idfs_tensor_fixed)\n",
    "                    composed_tensors['idf'][similarity][side][column] = \\\n",
    "                    dot_layer([embedded_tensors['idf'][similarity][side][column], idfs_tensor_normalized])\n",
    "                \n",
    "    # if enabled, compose embedding tensor using LSTM        \n",
    "    if 'lstm' in text_compositions:\n",
    "        for similarity in text_sim_metrics:\n",
    "            for side, column in it.product(sides, text_columns):\n",
    "                lstm_layer = LSTM(**lstm_args)\n",
    "                composed_tensors['lstm'][similarity][side][column] = \\\n",
    "                lstm_layer(embedded_tensors['lstm'][similarity][side][column])\n",
    "    \n",
    "    # if enambled, compose embedding tensor using bi-directional LSTM\n",
    "    if 'bi_lstm' in text_compositions:\n",
    "        for similarity in text_sim_metrics:\n",
    "            for side, column in it.product(sides, text_columns):\n",
    "                lstm_layer = Bidirectional(LSTM(**lstm_args), merge_mode='concat')\n",
    "                composed_tensors['bi_lstm'][similarity][side][column] = \\\n",
    "                lstm_layer(embedded_tensors['bi_lstm'][similarity][side][column])\n",
    "    \n",
    "    # maintain list of text-based similarities to calculate\n",
    "    similarity_layers = dict()\n",
    "    if 'cosine' in text_sim_metrics:\n",
    "        similarity_layer = Dot(axes=1, normalize=True)\n",
    "        similarity_layers['cosine'] = similarity_layer\n",
    "    if 'inverse_l1' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)))\n",
    "        similarity_layers['inverse_l1'] = similarity_layer\n",
    "    if 'inverse_l2' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: \\\n",
    "                                  K.exp(-K.sqrt(K.sum(K.pow(x[0]-x[1], 2), axis=1, keepdims=True))))\n",
    "        similarity_layers['inverse_l2'] = similarity_layer\n",
    "    \n",
    "    # for each attribute, calculate similarities between left and ride sides\n",
    "    similarity_tensors = list()\n",
    "    for composition, column, similarity in \\\n",
    "        it.product(text_compositions, text_columns, text_sim_metrics):     \n",
    "        similarity_tensor = similarity_layers[similarity]([composed_tensors[composition][similarity]['left'][column],\n",
    "                                                           composed_tensors[composition][similarity]['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "    \n",
    "    # reset similarity layer to empty so only numeric-based similarities are used\n",
    "    similarity_layers = list()\n",
    "    if 'scaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-2 * K.abs(x[0]-x[1]) / (x[0] + x[1])))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'unscaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.abs(x[0]-x[1])))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "        \n",
    "    for column, similarity_layer in it.product(numeric_columns, similarity_layers):\n",
    "        similarity_tensor = similarity_layer([input_tensors['left'][column],\n",
    "                                              input_tensors['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "    if 'min_max_ratio' in numeric_sim_metrics:\n",
    "        for column in numeric_columns:\n",
    "            num_concat = Concatenate(axis=-1)([input_tensors['left'][column], input_tensors['right'][column]])\n",
    "            similarity_layer = Lambda(lambda x: K.min(x, axis=1, keepdims=True) / \\\n",
    "                                                (K.max(x, axis=1, keepdims=True) + 1e-5))\n",
    "            similarity_tensors.append(similarity_layer(num_concat))\n",
    "    \n",
    "    # create input tensors from _isna attributes\n",
    "    input_isna_tensors = list()\n",
    "    if make_isna:\n",
    "        for side, column in it.product(sides, columns):\n",
    "            input_isna_tensors.append(Input(shape=(1,)))\n",
    "    \n",
    "    num_dense_inputs = len(similarity_tensors) + len(input_isna_tensors)\n",
    "    print('Number of inputs to dense layer: {}'.format(num_dense_inputs))\n",
    "    # concatenate similarity tensors with isna_tensors.\n",
    "    concatenated_tensors = Concatenate(axis=-1)(similarity_tensors + \\\n",
    "                                                input_isna_tensors)\n",
    "    \n",
    "    # create dense layers starting with concatenated tensors\n",
    "    dense_tensors = [concatenated_tensors]\n",
    "    for n_nodes in dense_nodes:\n",
    "        dense_tensor = Dense(n_nodes, activation='relu')(dense_tensors[-1])\n",
    "        if batch_norm and dropout:\n",
    "            dense_tensor_bn = BatchNormalization()(dense_tensor)\n",
    "            dense_tensor_dropout = Dropout(dropout)(dense_tensor_bn)\n",
    "            dense_tensors.append(dense_tensor_dropout)\n",
    "        else:\n",
    "            dense_tensors.append(dense_tensor)\n",
    "        dense_tensors.pop(0)\n",
    "        \n",
    "    output_tensors = Dense(2, activation='softmax')(dense_tensors[-1])\n",
    "    \n",
    "    product = list(it.product(sides, columns))\n",
    "    model = Model([input_tensors[s][tc] for s, tc in product] + input_isna_tensors,\n",
    "                  [output_tensors])\n",
    "    \n",
    "    return tuple([model] + list(packaged_data.values()) + [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "0",
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
