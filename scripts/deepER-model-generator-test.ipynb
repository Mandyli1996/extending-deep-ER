{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:37:49.543436Z",
     "start_time": "2018-04-01T20:37:47.158000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,\\\n",
    "                            average_precision_score, roc_auc_score,\\\n",
    "                            roc_curve, precision_recall_curve, confusion_matrix,\\\n",
    "                            accuracy_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from matplotlib import rcParams\n",
    "from importlib import reload\n",
    "from model_generator import deep_er_model_generator\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = 'times new roman'\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:44:46.357755Z",
     "start_time": "2018-04-01T20:44:29.463871Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(hp)\n",
    "\n",
    "with open('../data/converted/amazon-google/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "source_dir = os.path.join(data_dir,'split','amazon-google')\n",
    "data = hp.load_data(source_dir)\n",
    "\n",
    "datasets = ['train_1', 'val_1', 'test_1', 'train_2', 'val_2', 'test_2']\n",
    "\n",
    "data['train_2']['price'] = data['train_2']['price'].apply(hp.str_to_num)\n",
    "data['val_2']['price'] = data['val_2']['price'].apply(hp.str_to_num)\n",
    "data['test_2']['price'] = data['test_2']['price'].apply(hp.str_to_num)\n",
    "\n",
    "doc_freqs_1, doc_freqs_2 = hp.get_document_frequencies('../data/converted/amazon-google/', mapping=map)\n",
    "nan_idx = map['word2idx']['NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:08.826987Z",
     "start_time": "2018-04-01T21:08:02.029093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/derekzhao/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model, X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "deep_er_model_generator(data,\n",
    "                        embedding_file = '../data/converted/amazon-google/glove-300.matrix.npy',\n",
    "                        text_columns = ['name', 'description', 'manufacturer'],\n",
    "                        numeric_columns = ['price'],\n",
    "                        text_nan_idx=nan_idx,\n",
    "                        num_nan_val=0,\n",
    "                        text_sim_metrics=['cosine', 'inverse_l1'],\n",
    "                        text_compositions=['average', 'bi_lstm', 'idf'],\n",
    "                        numeric_sim_metrics=['scaled_inverse_lp', 'unscaled_inverse_lp', 'min_max_ratio'],\n",
    "                        dense_nodes=[20, 10],\n",
    "                        document_frequencies=(doc_freqs_1, doc_freqs_2),\n",
    "                        idf_smoothing=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:12.645116Z",
     "start_time": "2018-04-01T21:08:12.603115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:49.357323Z",
     "start_time": "2018-04-01T21:08:23.103407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105040 samples, validate on 13130 samples\n",
      "Epoch 1/10\n",
      "  2816/105040 [..............................] - ETA: 11:02 - loss: nan - acc: 0.0014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e622b5286303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=10, batch_size=128,\n\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/entity-resolution/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T23:22:02.598485Z",
     "start_time": "2018-04-01T23:22:02.580287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_generator.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Lambda, Dot, Concatenate, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def deep_er_model_generator(data_dict,\n",
    "                            embedding_file,\n",
    "                            padding_limit = 100,\n",
    "                            mask_zero = True,\n",
    "                            embedding_trainable = False,\n",
    "                            text_columns = list(), \n",
    "                            numeric_columns = list(),\n",
    "                            make_isna = True,\n",
    "                            text_nan_idx = None,\n",
    "                            num_nan_val = None,\n",
    "                            text_compositions = ['average'],\n",
    "                            text_sim_metrics = ['cosine'],\n",
    "                            numeric_sim_metrics = ['unscaled_inverse_lp'],\n",
    "                            dense_nodes = [10],\n",
    "                            lstm_args = dict(units=50),\n",
    "                            document_frequencies = None,\n",
    "                            idf_smoothing = 2):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of paired split DataFrames and returns a DeepER \n",
    "    model with data formatted for said model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        A dictionary of dataframes (pd.DataFrame) stored with the following\n",
    "        keys: train_1, val_1, test_1, train_2, val_2, test_2\n",
    "    embedding_file : str\n",
    "        The location and name of numpy matrix containing word vector\n",
    "        embeddings.\n",
    "    padding_limit : int, optional\n",
    "        The maximum length of any text sequence. For any text attribute whose\n",
    "        max length is below padding_limit, the max length will be used.\n",
    "        Otherwise, padding_limit will be used to both pad and truncuate\n",
    "        text sequences for that attribute.\n",
    "    mask_zero : bool, optional\n",
    "        Whether to ignore text sequence indices with value of 0. Useful for\n",
    "        LSTM's and variable length inputs.\n",
    "    embedding_trainable: bool, optional\n",
    "        Whether to allow the embedding layer to be fine tuned.\n",
    "    text_columns : list of strings, optional\n",
    "        A list of names of text-based attributes\n",
    "    numeric_columns : list of strings, optional\n",
    "        A list of names of numeric attributes\n",
    "    make_isna: bool, optional\n",
    "        Whether to create new attributes indicating the presence of null values\n",
    "        for each original attribute.\n",
    "    text_nan_idx : int, optional\n",
    "        The index corresponding to NaN values in text-based attributes.\n",
    "    num_nan_val : int, optional\n",
    "        The value corresponding to NaN values in numeric attributes.\n",
    "    text_compositions : list of strings, optional\n",
    "        List of composition methods to be applied to embedded text attributes.\n",
    "        Valid options are :\n",
    "            - average : a simple average of all embedded vectors\n",
    "            - idf : an average of all embedded vectors weighted by normalized\n",
    "                    inverse document frequency\n",
    "    text_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each text-based attribute.\n",
    "        Valid options are :\n",
    "            - cosine\n",
    "            - inverse_l1 : e^-[l1_distance]\n",
    "            - inverse_l2 : e^-[l2_distance]\n",
    "    numeric_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each numeric attribute.\n",
    "        Valid options are :\n",
    "            - scaled_inverse_lp : e^[-2(abs_diff)/sum]\n",
    "            - unscaled_inverse_lp : e^[-abs_diff]\n",
    "            - min_max_ratio : min / max\n",
    "    dense_nodes : list of ints, optional\n",
    "        Specifies topology of hidden dense layers\n",
    "    lstm_args = dict, optional\n",
    "        Keyword arguments for LSTM layer\n",
    "    document_frequencies = tuple of length 2, optional\n",
    "        Tuple of two lists of document frequencies, left side then right\n",
    "    idf_smoothing : int, optional\n",
    "        un-normalized idf = 1 / df ^ (1 / idf_smoothing)\n",
    "        Higher values means that high document frequency words are penalized\n",
    "        less.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### DATA PROCESSING ###\n",
    "    # initialize an empty dictionary for storing all data\n",
    "    # dictionary structure will be data[split][side][column]\n",
    "    sides = ['left', 'right']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = dict()\n",
    "    for split in splits:\n",
    "        data[split] = dict()\n",
    "        for side in sides:\n",
    "            data[split][side] = dict()\n",
    "            \n",
    "    columns = text_columns + numeric_columns\n",
    "    \n",
    "    # separate each feature into its own dictionary entry\n",
    "    for column in columns:\n",
    "        data['train']['left'][column] = data_dict['train_1'][column]\n",
    "        data['train']['right'][column] = data_dict['train_2'][column]\n",
    "\n",
    "        data['val']['left'][column] = data_dict['val_1'][column]\n",
    "        data['val']['right'][column] = data_dict['val_2'][column]\n",
    "\n",
    "        data['test']['left'][column] = data_dict['test_1'][column]\n",
    "        data['test']['right'][column] = data_dict['test_2'][column]\n",
    "    \n",
    "    # if enabled, create a binary column for each feature indicating whether\n",
    "    # it contains a missing value. for text data, this will be a list with\n",
    "    # a single index representing the 'NaN' token. for numeric data, this will\n",
    "    # likely be a 0.\n",
    "    if make_isna:\n",
    "        for split, side, column in it.product(splits, sides, text_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == [text_nan_idx])\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            isna_column = column + '_isna'\n",
    "            data[split][side][isna_column] = isna\n",
    "        for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == num_nan_val)\n",
    "            isna_column = column + '_isna'\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            data[split][side][isna_column] = isna\n",
    "    \n",
    "    # pad each text column according to the length of its longest entry in\n",
    "    # both datasets\n",
    "    maxlen = dict()\n",
    "    for column in text_columns:\n",
    "        maxlen_left = data['train']['left'][column].apply(lambda x: len(x)).max()\n",
    "        maxlen_right = data['train']['right'][column].apply(lambda x: len(x)).max()\n",
    "        maxlength = min(padding_limit, max(maxlen_left, maxlen_right))\n",
    "        for split, side in it.product(splits, sides):\n",
    "            data[split][side][column] = pad_sequences(data[split][side][column],\n",
    "                                                      maxlen=maxlength,\n",
    "                                                      padding='post',\n",
    "                                                      truncating='post')\n",
    "        maxlen[column] = maxlength\n",
    "    \n",
    "    # convert all numeric features to float and reshape to be 2-dimensional\n",
    "    for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "        feature = data[split][side][column]\n",
    "        feature = feature.values.astype(np.float32).reshape(-1,1)\n",
    "        data[split][side][column] = feature\n",
    "            \n",
    "    # format X values for each split as a list of 2-dimensional arrays\n",
    "    packaged_data = OrderedDict()\n",
    "    for split in splits:\n",
    "        packaged_data[split] = list()\n",
    "        for side, column in it.product(sides, columns):\n",
    "            packaged_data[split].append(data[split][side][column])\n",
    "        if make_isna:\n",
    "            for side, column in it.product(sides, columns):\n",
    "                packaged_data[split].append(data[split][side][column + '_isna'])\n",
    "    \n",
    "    # convert y-values\n",
    "    y_train = to_categorical(data_dict['train_y'])\n",
    "    y_val = to_categorical(data_dict['val_y'])\n",
    "    y_test = to_categorical(data_dict['test_y'])\n",
    "    \n",
    "    ### MODEL BUILDING ###\n",
    "    \n",
    "    # each attribute of each side is its own input tensor\n",
    "    # text input tensors for both sides are created before numeric input tensors\n",
    "    input_tensors = dict(left=dict(), right=dict())\n",
    "    for side, column in it.product(sides, text_columns):\n",
    "        input_tensors[side][column] = Input(shape=(maxlen[column],))\n",
    "        \n",
    "    for side, column in it.product(sides, numeric_columns):\n",
    "        input_tensors[side][column] = Input(shape=(1,))\n",
    "    \n",
    "    # create a single embedding layer for text input tensors\n",
    "    embedding_matrix = np.load(embedding_file)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=embedding_trainable,\n",
    "                                mask_zero=mask_zero)\n",
    "    \n",
    "    \n",
    "    # use embedding_layer ot convert text input tensors to embedded tensors\n",
    "    # and store in dictionary.\n",
    "    # an embedding tensor will have shape n_words x n_embedding_dimensions\n",
    "    embedded_tensors = dict(left=dict(), right=dict())\n",
    "    for side, column in it.product(sides, text_columns):\n",
    "        embedded_tensors[side][column] = embedding_layer(input_tensors[side][column])\n",
    "    \n",
    "    # initialize dictionary for storing a composition tensor for each embedding tensor\n",
    "    composed_tensors = dict()\n",
    "    for composition in text_compositions:\n",
    "        composed_tensors[composition] = dict()\n",
    "        for side in sides:\n",
    "            composed_tensors[composition][side] = dict()\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by averaging all embeddings\n",
    "    if 'average' in text_compositions:\n",
    "        averaging_layer = Lambda(lambda x: K.mean(x, axis=1), output_shape=(maxlen[column],))\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            composed_tensors['average'][side][column] = averaging_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by taking a weighted average of all\n",
    "    # embeddings. \n",
    "    if 'idf' in text_compositions:\n",
    "        # store document frequency constants for each side\n",
    "        dfs_constant = dict()\n",
    "        dfs_constant['left'] = K.constant(document_frequencies[0])\n",
    "        dfs_constant['right'] = K.constant(document_frequencies[1])\n",
    "        \n",
    "        # a selection layer uses an input tensor as indices to select\n",
    "        # document frequencies from dfs_constant\n",
    "        dfs_selection_layer = dict()\n",
    "        \n",
    "        # a conversion layer converts a tensor of selected document frequencies\n",
    "        # to a tensor of inverse document frequencies. the larger the DF,\n",
    "        # the smaller the inverse, the smallness of which is controlled by\n",
    "        # idf_smoothing\n",
    "        idf_conversion_layer = Lambda(lambda x: 1 / (K.pow(x, 1/idf_smoothing)))\n",
    "        \n",
    "        # document frequencies of 0 will result in IDF's of inf. these should\n",
    "        # be converted back to 0's.\n",
    "        idf_fix_layer = Lambda(lambda x: tf.where(tf.is_inf(x), tf.zeros_like(x), x))\n",
    "        \n",
    "        # for each IDF tensor, scale its values so they sum to 1\n",
    "        idf_normalization_layer = Lambda(lambda x: x / K.expand_dims(K.sum(x, axis=1), axis=1))\n",
    "        \n",
    "        # take dot product between embedding tensor vectors and IDF weights\n",
    "        dot_layer = Dot(axes=1)\n",
    "        \n",
    "        for side in sides:\n",
    "            dfs_selection_layer[side] = Lambda(lambda x: K.gather(dfs_constant[side], K.cast(x, tf.int32)))\n",
    "            for column in text_columns:                \n",
    "                dfs_tensor = dfs_selection_layer[side](input_tensors[side][column])\n",
    "                idfs_tensor = idf_conversion_layer(dfs_tensor)\n",
    "                idfs_tensor_fixed = idf_fix_layer(idfs_tensor)\n",
    "                idfs_tensor_normalized = idf_normalization_layer(idfs_tensor_fixed)\n",
    "                composed_tensors['idf'][side][column] = dot_layer([embedded_tensors[side][column],\n",
    "                                                                   idfs_tensor_normalized])\n",
    "                \n",
    "    # if enabled, compose embedding tensor using LSTM        \n",
    "    if 'lstm' in text_compositions:\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            lstm_layer = LSTM(**lstm_args)\n",
    "            composed_tensors['lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # if enambled, compose embedding tensor using bi-directional LSTM\n",
    "    if 'bi_lstm' in text_compositions:\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            lstm_layer = Bidirectional(LSTM(**lstm_args), merge_mode='concat')\n",
    "            composed_tensors['bi_lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # maintain list of text-based similarities to calculate\n",
    "    similarity_layers = list()\n",
    "    if 'cosine' in text_sim_metrics:\n",
    "        similarity_layer = Dot(axes=1, normalize=True)\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'inverse_l1' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'inverse_l2' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: \\\n",
    "                                  K.exp(-K.sqrt(K.sum(K.pow(x[0]-x[1], 2), axis=1, keepdims=True))))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    \n",
    "    # for each attribute, calculate similarities between left and ride sides\n",
    "    similarity_tensors = list()\n",
    "    for composition, column, similarity_layer in \\\n",
    "        it.product(text_compositions, text_columns, similarity_layers):        \n",
    "        similarity_tensor = similarity_layer([composed_tensors[composition]['left'][column],\n",
    "                                              composed_tensors[composition]['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "    \n",
    "    # reset similarity layer to empty so only numeric-based similarities are used\n",
    "    similarity_layers = list()\n",
    "    if 'scaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-2 * K.abs(x[0]-x[1]) / (x[0] + x[1])))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'unscaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.abs(x[0]-x[1])))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "        \n",
    "    for column, similarity_layer in it.product(numeric_columns, similarity_layers):\n",
    "        similarity_tensor = similarity_layer([input_tensors['left'][column],\n",
    "                                              input_tensors['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "    if 'min_max_ratio' in numeric_sim_metrics:\n",
    "        for column in numeric_columns:\n",
    "            num_concat = Concatenate(axis=-1)([input_tensors['left'][column], input_tensors['right'][column]])\n",
    "            similarity_layer = Lambda(lambda x: K.min(x, axis=1, keepdims=True) / \\\n",
    "                                                (K.max(x, axis=1, keepdims=True) + 1e-5))\n",
    "            similarity_tensors.append(similarity_layer(num_concat))\n",
    "    \n",
    "    # create input tensors from _isna attributes\n",
    "    input_isna_tensors = list()\n",
    "    if make_isna:\n",
    "        for side, column in it.product(sides, columns):\n",
    "            input_isna_tensors.append(Input(shape=(1,)))\n",
    "    \n",
    "    # concatenate similarity tensors with isna_tensors.\n",
    "    concatenated_tensors = Concatenate(axis=-1)(similarity_tensors + \\\n",
    "                                                input_isna_tensors)\n",
    "    \n",
    "    # create dense layers starting with concatenated tensors\n",
    "    dense_tensors = [concatenated_tensors]\n",
    "    for n_nodes in dense_nodes:\n",
    "        dense_tensors.append(Dense(n_nodes, activation='relu')(dense_tensors[-1]))\n",
    "        dense_tensors.pop(0)\n",
    "        \n",
    "    output_tensors = Dense(2, activation='softmax')(dense_tensors[-1])\n",
    "    \n",
    "    product = list(it.product(sides, columns))\n",
    "    model = Model([input_tensors[s][tc] for s, tc in product] + input_isna_tensors,\n",
    "                  [output_tensors])\n",
    "    \n",
    "    return tuple([model] + list(packaged_data.values()) + [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "0",
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
