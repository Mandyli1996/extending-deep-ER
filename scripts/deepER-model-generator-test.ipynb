{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:37:49.543436Z",
     "start_time": "2018-04-01T20:37:47.158000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,\\\n",
    "                            average_precision_score, roc_auc_score,\\\n",
    "                            roc_curve, precision_recall_curve, confusion_matrix,\\\n",
    "                            accuracy_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from matplotlib import rcParams\n",
    "from importlib import reload\n",
    "from model_generator import deep_er_model_generator\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = 'times new roman'\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T20:44:46.357755Z",
     "start_time": "2018-04-01T20:44:29.463871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(hp)\n",
    "\n",
    "with open('../data/converted/amazon-google/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "source_dir = os.path.join(data_dir,'split','amazon-google')\n",
    "data = hp.load_data(source_dir)\n",
    "\n",
    "datasets = ['train_1', 'val_1', 'test_1', 'train_2', 'val_2', 'test_2']\n",
    "\n",
    "data['train_2']['price'] = data['train_2']['price'].apply(hp.str_to_num)\n",
    "data['val_2']['price'] = data['val_2']['price'].apply(hp.str_to_num)\n",
    "data['test_2']['price'] = data['test_2']['price'].apply(hp.str_to_num)\n",
    "\n",
    "doc_freqs_1, doc_freqs_2 = hp.get_document_frequencies('../data/converted/amazon-google/', mapping=map)\n",
    "nan_idx = map['word2idx']['NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:08.826987Z",
     "start_time": "2018-04-01T21:08:02.029093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs to dense layer: 123\n"
     ]
    }
   ],
   "source": [
    "model, X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "deep_er_model_generator(data,\n",
    "                        embedding_file = '../data/converted/amazon-google/glove-300.matrix.npy',\n",
    "                        text_columns = ['name', 'description'],\n",
    "                        numeric_columns = ['price'],\n",
    "                        text_nan_idx=nan_idx,\n",
    "                        num_nan_val=0,\n",
    "                        text_sim_metrics=['cosine', 'inverse_l1'],\n",
    "                        text_compositions=['average', 'bi_lstm', 'idf'],\n",
    "                        numeric_sim_metrics=['scaled_inverse_lp', 'unscaled_inverse_lp', 'min_max_ratio'],\n",
    "                        dense_nodes=[64, 16, 8],\n",
    "                        document_frequencies=(doc_freqs_1, doc_freqs_2),\n",
    "                        idf_smoothing=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:12.645116Z",
     "start_time": "2018-04-01T21:08:12.603115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T21:08:49.357323Z",
     "start_time": "2018-04-01T21:08:23.103407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104000 samples, validate on 13000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,512,300]\n\t [[Node: bidirectional_3_3/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_3_3/Gather, bidirectional_3/transpose/perm)]]\n\t [[Node: bidirectional_3_3/while/Identity/_541 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3122_bidirectional_3_3/while/Identity\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopbidirectional_3_3/while/Const_5/_234)]]\n\nCaused by op 'bidirectional_3_3/transpose', defined at:\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-b67c3f442b35>\", line 12, in <module>\n    idf_smoothing=2)\n  File \"<ipython-input-17-2fa76bdee30b>\", line 279, in deep_er_model_generator\n    composed_tensors['bi_lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/wrappers.py\", line 324, in __call__\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/wrappers.py\", line 384, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2063, in call\n    initial_state=initial_state)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2590, in rnn\n    inputs = tf.transpose(inputs, (axes))\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1336, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5694, in transpose\n    \"Transpose\", x=x, perm=perm, name=name)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,512,300]\n\t [[Node: bidirectional_3_3/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_3_3/Gather, bidirectional_3/transpose/perm)]]\n\t [[Node: bidirectional_3_3/while/Identity/_541 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3122_bidirectional_3_3/while/Identity\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopbidirectional_3_3/while/Const_5/_234)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,512,300]\n\t [[Node: bidirectional_3_3/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_3_3/Gather, bidirectional_3/transpose/perm)]]\n\t [[Node: bidirectional_3_3/while/Identity/_541 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3122_bidirectional_3_3/while/Identity\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopbidirectional_3_3/while/Const_5/_234)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d530f9faf626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=1, batch_size=512,\n\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,512,300]\n\t [[Node: bidirectional_3_3/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_3_3/Gather, bidirectional_3/transpose/perm)]]\n\t [[Node: bidirectional_3_3/while/Identity/_541 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3122_bidirectional_3_3/while/Identity\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopbidirectional_3_3/while/Const_5/_234)]]\n\nCaused by op 'bidirectional_3_3/transpose', defined at:\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-b67c3f442b35>\", line 12, in <module>\n    idf_smoothing=2)\n  File \"<ipython-input-17-2fa76bdee30b>\", line 279, in deep_er_model_generator\n    composed_tensors['bi_lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/wrappers.py\", line 324, in __call__\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/wrappers.py\", line 384, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2063, in call\n    initial_state=initial_state)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2590, in rnn\n    inputs = tf.transpose(inputs, (axes))\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1336, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5694, in transpose\n    \"Transpose\", x=x, perm=perm, name=name)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/python-36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,512,300]\n\t [[Node: bidirectional_3_3/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_3_3/Gather, bidirectional_3/transpose/perm)]]\n\t [[Node: bidirectional_3_3/while/Identity/_541 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3122_bidirectional_3_3/while/Identity\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopbidirectional_3_3/while/Const_5/_234)]]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1, batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T22:43:45.050922Z",
     "start_time": "2018-04-12T22:43:45.024111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_generator.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helpers as hp\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Lambda, Dot, Concatenate, \\\n",
    "                         Bidirectional, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def deep_er_model_generator(data_dict,\n",
    "                            embedding_file,\n",
    "                            padding_limit = 100,\n",
    "                            mask_zero = True,\n",
    "                            embedding_trainable = False,\n",
    "                            text_columns = list(), \n",
    "                            numeric_columns = list(),\n",
    "                            make_isna = True,\n",
    "                            text_nan_idx = None,\n",
    "                            num_nan_val = None,\n",
    "                            text_compositions = ['average'],\n",
    "                            text_sim_metrics = ['cosine'],\n",
    "                            numeric_sim_metrics = ['unscaled_inverse_lp'],\n",
    "                            dense_nodes = [10],\n",
    "                            lstm_args = dict(units=50),\n",
    "                            document_frequencies = None,\n",
    "                            idf_smoothing = 2,\n",
    "                            batch_norm = False,\n",
    "                            dropout = 0,\n",
    "                            shared_lstm = True,\n",
    "                            debug = False):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of paired split DataFrames and returns a DeepER \n",
    "    model with data formatted for said model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        A dictionary of dataframes (pd.DataFrame) stored with the following\n",
    "        keys: train_1, val_1, test_1, train_2, val_2, test_2\n",
    "    embedding_file : str\n",
    "        The location and name of numpy matrix containing word vector\n",
    "        embeddings.\n",
    "    padding_limit : int, optional\n",
    "        The maximum length of any text sequence. For any text attribute whose\n",
    "        max length is below padding_limit, the max length will be used.\n",
    "        Otherwise, padding_limit will be used to both pad and truncuate\n",
    "        text sequences for that attribute.\n",
    "    mask_zero : bool, optional\n",
    "        Whether to ignore text sequence indices with value of 0. Useful for\n",
    "        LSTM's and variable length inputs.\n",
    "    embedding_trainable: bool, optional\n",
    "        Whether to allow the embedding layer to be fine tuned.\n",
    "    text_columns : list of strings, optional\n",
    "        A list of names of text-based attributes\n",
    "    numeric_columns : list of strings, optional\n",
    "        A list of names of numeric attributes\n",
    "    make_isna: bool, optional\n",
    "        Whether to create new attributes indicating the presence of null values\n",
    "        for each original attribute.\n",
    "    text_nan_idx : int, optional\n",
    "        The index corresponding to NaN values in text-based attributes.\n",
    "    num_nan_val : int, optional\n",
    "        The value corresponding to NaN values in numeric attributes.\n",
    "    text_compositions : list of strings, optional\n",
    "        List of composition methods to be applied to embedded text attributes.\n",
    "        Valid options are :\n",
    "            - average : a simple average of all embedded vectors\n",
    "            - idf : an average of all embedded vectors weighted by normalized\n",
    "                    inverse document frequency\n",
    "    text_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each text-based attribute.\n",
    "        Valid options are :\n",
    "            - cosine\n",
    "            - inverse_l1 : e^-[l1_distance]\n",
    "            - inverse_l2 : e^-[l2_distance]\n",
    "    numeric_sim_metrics : list of strings, optional\n",
    "        List of similarity metrics to be computed for each numeric attribute.\n",
    "        Valid options are :\n",
    "            - scaled_inverse_lp : e^[-2(abs_diff)/sum]\n",
    "            - unscaled_inverse_lp : e^[-abs_diff]\n",
    "            - min_max_ratio : min / max\n",
    "    dense_nodes : list of ints, optional\n",
    "        Specifies topology of hidden dense layers\n",
    "    lstm_args = dict, optional\n",
    "        Keyword arguments for LSTM layer\n",
    "    document_frequencies = tuple of length 2, optional\n",
    "        Tuple of two lists of document frequencies, left side then right\n",
    "    idf_smoothing : int, optional\n",
    "        un-normalized idf = 1 / df ^ (1 / idf_smoothing)\n",
    "        Higher values means that high document frequency words are penalized\n",
    "        less.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### DATA PROCESSING ###\n",
    "    # initialize an empty dictionary for storing all data\n",
    "    # dictionary structure will be data[split][side][column]\n",
    "    sides = ['left', 'right']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = dict()\n",
    "    for split in splits:\n",
    "        data[split] = dict()\n",
    "        for side in sides:\n",
    "            data[split][side] = dict()\n",
    "            \n",
    "    columns = text_columns + numeric_columns\n",
    "    \n",
    "    # separate each feature into its own dictionary entry\n",
    "    for column in columns:\n",
    "        data['train']['left'][column] = data_dict['train_1'][column]\n",
    "        data['train']['right'][column] = data_dict['train_2'][column]\n",
    "\n",
    "        data['val']['left'][column] = data_dict['val_1'][column]\n",
    "        data['val']['right'][column] = data_dict['val_2'][column]\n",
    "\n",
    "        data['test']['left'][column] = data_dict['test_1'][column]\n",
    "        data['test']['right'][column] = data_dict['test_2'][column]\n",
    "    \n",
    "    # if enabled, create a binary column for each feature indicating whether\n",
    "    # it contains a missing value. for text data, this will be a list with\n",
    "    # a single index representing the 'NaN' token. for numeric data, this will\n",
    "    # likely be a 0.\n",
    "    if make_isna:\n",
    "        for split, side, column in it.product(splits, sides, text_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == [text_nan_idx])\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            isna_column = column + '_isna'\n",
    "            data[split][side][isna_column] = isna\n",
    "        for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "            isna = data[split][side][column].apply(lambda x: x == num_nan_val)\n",
    "            isna_column = column + '_isna'\n",
    "            isna = isna.values.astype(np.float32).reshape(-1, 1)\n",
    "            data[split][side][isna_column] = isna\n",
    "    \n",
    "    # pad each text column according to the length of its longest entry in\n",
    "    # both datasets\n",
    "    maxlen = dict()\n",
    "    for column in text_columns:\n",
    "        maxlen_left = data['train']['left'][column].apply(lambda x: len(x)).max()\n",
    "        maxlen_right = data['train']['right'][column].apply(lambda x: len(x)).max()\n",
    "        maxlength = min(padding_limit, max(maxlen_left, maxlen_right))\n",
    "        for split, side in it.product(splits, sides):\n",
    "            data[split][side][column] = pad_sequences(data[split][side][column],\n",
    "                                                      maxlen=maxlength,\n",
    "                                                      padding='post',\n",
    "                                                      truncating='post')\n",
    "        maxlen[column] = maxlength\n",
    "    \n",
    "    # convert all numeric features to float and reshape to be 2-dimensional\n",
    "    for split, side, column in it.product(splits, sides, numeric_columns):\n",
    "        feature = data[split][side][column]\n",
    "        feature = feature.values.astype(np.float32).reshape(-1,1)\n",
    "        data[split][side][column] = feature\n",
    "            \n",
    "    # format X values for each split as a list of 2-dimensional arrays\n",
    "    packaged_data = OrderedDict()\n",
    "    for split in splits:\n",
    "        packaged_data[split] = list()\n",
    "        for side, column in it.product(sides, columns):\n",
    "            packaged_data[split].append(data[split][side][column])\n",
    "        if make_isna:\n",
    "            for side, column in it.product(sides, columns):\n",
    "                packaged_data[split].append(data[split][side][column + '_isna'])\n",
    "    \n",
    "    # convert y-values\n",
    "    y_train = to_categorical(data_dict['train_y'])\n",
    "    y_val = to_categorical(data_dict['val_y'])\n",
    "    y_test = to_categorical(data_dict['test_y'])\n",
    "    \n",
    "    ### MODEL BUILDING ###\n",
    "    \n",
    "    # each attribute of each side is its own input tensor\n",
    "    # text input tensors for both sides are created before numeric input tensors\n",
    "    input_tensors = dict(left=dict(), right=dict())\n",
    "    for side, column in it.product(sides, text_columns):\n",
    "        input_tensors[side][column] = Input(shape=(maxlen[column],))\n",
    "        \n",
    "    for side, column in it.product(sides, numeric_columns):\n",
    "        input_tensors[side][column] = Input(shape=(1,))\n",
    "    \n",
    "    # create a single embedding layer for text input tensors\n",
    "    embedding_matrix = np.load(embedding_file)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=embedding_trainable,\n",
    "                                mask_zero=mask_zero)\n",
    "    \n",
    "    \n",
    "    # use embedding_layer ot convert text input tensors to embedded tensors\n",
    "    # and store in dictionary.\n",
    "    # an embedding tensor will have shape n_words x n_embedding_dimensions\n",
    "    embedded_tensors = dict(left=dict(), right=dict())\n",
    "    for side, column in it.product(sides, text_columns):\n",
    "        embedded_tensors[side][column] = embedding_layer(input_tensors[side][column])\n",
    "    \n",
    "    # initialize dictionary for storing a composition tensor for each embedding tensor\n",
    "    composed_tensors = dict()\n",
    "    for composition in text_compositions:\n",
    "        composed_tensors[composition] = dict()\n",
    "        for side in sides:\n",
    "            composed_tensors[composition][side] = dict()\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by averaging all embeddings\n",
    "    if 'average' in text_compositions:\n",
    "        averaging_layer = Lambda(lambda x: K.mean(x, axis=1), output_shape=(maxlen[column],))\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            composed_tensors['average'][side][column] = averaging_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # if enabled, reduce each embedding tensor to a quasi-1-dimensional tensor\n",
    "    # with shape 1 x n_embedding_dimensions by taking a weighted average of all\n",
    "    # embeddings. \n",
    "    if 'idf' in text_compositions:\n",
    "        # store document frequency constants for each side\n",
    "        dfs_constant = dict()\n",
    "        dfs_constant['left'] = K.constant(document_frequencies[0])\n",
    "        dfs_constant['right'] = K.constant(document_frequencies[1])\n",
    "        \n",
    "        # a selection layer uses an input tensor as indices to select\n",
    "        # document frequencies from dfs_constant\n",
    "        dfs_selection_layer = dict()\n",
    "        \n",
    "        # a conversion layer converts a tensor of selected document frequencies\n",
    "        # to a tensor of inverse document frequencies. the larger the DF,\n",
    "        # the smaller the inverse, the smallness of which is controlled by\n",
    "        # idf_smoothing\n",
    "        idf_conversion_layer = Lambda(lambda x: 1 / (K.pow(x, 1/idf_smoothing)))\n",
    "        \n",
    "        # document frequencies of 0 will result in IDF's of inf. these should\n",
    "        # be converted back to 0's.\n",
    "        idf_fix_layer = Lambda(lambda x: tf.where(tf.is_inf(x), tf.zeros_like(x), x))\n",
    "        \n",
    "        # for each IDF tensor, scale its values so they sum to 1\n",
    "        idf_normalization_layer = Lambda(lambda x: x / K.expand_dims(K.sum(x, axis=1), axis=1))\n",
    "        \n",
    "        # take dot product between embedding tensor vectors and IDF weights\n",
    "        dot_layer = Dot(axes=1)\n",
    "        \n",
    "        for side in sides:\n",
    "            dfs_selection_layer[side] = Lambda(lambda x: K.gather(dfs_constant[side], K.cast(x, tf.int32)))\n",
    "            for column in text_columns:                \n",
    "                dfs_tensor = dfs_selection_layer[side](input_tensors[side][column])\n",
    "                idfs_tensor = idf_conversion_layer(dfs_tensor)\n",
    "                idfs_tensor_fixed = idf_fix_layer(idfs_tensor)\n",
    "                idfs_tensor_normalized = idf_normalization_layer(idfs_tensor_fixed)\n",
    "                composed_tensors['idf'][side][column] = dot_layer([embedded_tensors[side][column],\n",
    "                                                                   idfs_tensor_normalized])\n",
    "                \n",
    "    # if enabled, compose embedding tensor using shared LSTM        \n",
    "    if 'lstm' in text_compositions:\n",
    "        if shared_lstm:\n",
    "            lstm_layer = LSTM(**lstm_args)\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            if not shared_lstm:\n",
    "                lstm_layer = LSTM(**lstm_args)\n",
    "            composed_tensors['lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # if enambled, compose embedding tensor using bi-directional LSTM\n",
    "    if 'bi_lstm' in text_compositions:\n",
    "        if shared_lstm:\n",
    "            lstm_layer = lstm_layer = Bidirectional(LSTM(**lstm_args), merge_mode='concat')\n",
    "        for side, column in it.product(sides, text_columns):\n",
    "            if not shared_lstm:\n",
    "                lstm_layer = Bidirectional(LSTM(**lstm_args), merge_mode='concat')\n",
    "            composed_tensors['bi_lstm'][side][column] = lstm_layer(embedded_tensors[side][column])\n",
    "    \n",
    "    # maintain list of text-based similarities to calculate\n",
    "    similarity_layers = list()\n",
    "    if 'cosine' in text_sim_metrics:\n",
    "        similarity_layer = Dot(axes=1, normalize=True)\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'inverse_l1' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'inverse_l2' in text_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: \\\n",
    "                                  K.exp(-K.sqrt(K.sum(K.pow(x[0]-x[1], 2), axis=1, keepdims=True))))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    \n",
    "    # for each attribute, calculate similarities between left and ride sides\n",
    "    similarity_tensors = list()\n",
    "    for composition, column, similarity_layer in \\\n",
    "        it.product(text_compositions, text_columns, similarity_layers):        \n",
    "        similarity_tensor = similarity_layer([composed_tensors[composition]['left'][column],\n",
    "                                              composed_tensors[composition]['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "        \n",
    "    if 'bi_lstm' in text_compositions:\n",
    "        difference_layer = Lambda(lambda x: K.abs(x[0]-x[1]))\n",
    "        hadamard_layer = Lambda(lambda x: x[0] * x[1])\n",
    "        for column in text_columns:\n",
    "            difference_tensor = difference_layer([composed_tensors['bi_lstm']['left'][column],\n",
    "                                                  composed_tensors['bi_lstm']['right'][column]])\n",
    "            hadamard_tensor = hadamard_layer([composed_tensors['bi_lstm']['left'][column],\n",
    "                                              composed_tensors['bi_lstm']['right'][column]])\n",
    "            similarity_tensors.extend([difference_tensor, hadamard_tensor])\n",
    "    \n",
    "    # reset similarity layer to empty so only numeric-based similarities are used\n",
    "    similarity_layers = list()\n",
    "    if 'scaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-2 * K.abs(x[0]-x[1]) / (x[0] + x[1] + 1e-5)))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "    if 'unscaled_inverse_lp' in numeric_sim_metrics:\n",
    "        similarity_layer = Lambda(lambda x: K.exp(-K.abs(x[0]-x[1])))\n",
    "        similarity_layers.append(similarity_layer)\n",
    "        \n",
    "    for column, similarity_layer in it.product(numeric_columns, similarity_layers):\n",
    "        similarity_tensor = similarity_layer([input_tensors['left'][column],\n",
    "                                              input_tensors['right'][column]])\n",
    "        similarity_tensors.append(similarity_tensor)\n",
    "    if 'min_max_ratio' in numeric_sim_metrics:\n",
    "        for column in numeric_columns:\n",
    "            num_concat = Concatenate(axis=-1)([input_tensors['left'][column], input_tensors['right'][column]])\n",
    "            similarity_layer = Lambda(lambda x: K.min(x, axis=1, keepdims=True) / \\\n",
    "                                                (K.max(x, axis=1, keepdims=True) + 1e-5))\n",
    "            similarity_tensors.append(similarity_layer(num_concat))\n",
    "    \n",
    "    # create input tensors from _isna attributes\n",
    "    input_isna_tensors = list()\n",
    "    if make_isna:\n",
    "        for side, column in it.product(sides, columns):\n",
    "            input_isna_tensors.append(Input(shape=(1,)))\n",
    "    \n",
    "    num_dense_inputs = len(similarity_tensors) + len(input_isna_tensors) \n",
    "    if 'lstm ' in text_compositions or 'bi_lstm' in text_compositions:\n",
    "        num_dense_inputs += lstm_args['units'] * len(text_columns)\n",
    "    print('Number of inputs to dense layer: {}'.format(num_dense_inputs))\n",
    "    # concatenate similarity tensors with isna_tensors.\n",
    "    concatenated_tensors = Concatenate(axis=-1)(similarity_tensors + \\\n",
    "                                                input_isna_tensors)\n",
    "    \n",
    "    # create dense layers starting with concatenated tensors\n",
    "    dense_tensors = [concatenated_tensors]\n",
    "    for n_nodes in dense_nodes:\n",
    "        dense_tensor = Dense(n_nodes, activation='relu')(dense_tensors[-1])\n",
    "        if batch_norm and dropout:\n",
    "            dense_tensor_bn = BatchNormalization()(dense_tensor)\n",
    "            dense_tensor_dropout = Dropout(dropout)(dense_tensor_bn)\n",
    "            dense_tensors.append(dense_tensor_dropout)\n",
    "        else:\n",
    "            dense_tensors.append(dense_tensor)\n",
    "        dense_tensors.pop(0)\n",
    "        \n",
    "    output_tensors = Dense(2, activation='softmax')(dense_tensors[-1])\n",
    "    \n",
    "    product = list(it.product(sides, columns))\n",
    "    if not debug:\n",
    "        model = Model([input_tensors[s][tc] for s, tc in product] + input_isna_tensors,\n",
    "                      [output_tensors])\n",
    "    else:\n",
    "        model = Model([input_tensors[s][tc] for s, tc in product] + input_isna_tensors,\n",
    "                      [embedded_tensors['left'][text_columns[0]]])\n",
    "    \n",
    "    return tuple([model] + list(packaged_data.values()) + [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "0",
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
