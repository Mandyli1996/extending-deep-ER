{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-google: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T03:35:22.736770Z",
     "start_time": "2018-04-02T03:34:52.765031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 matched pairs detected. Creating 128700 non-matched pairs.\n",
      "Training set contains 104000 instances\n",
      "Validation set contains 13000 instances\n",
      "Test set contains 13000 instances\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/amazon-google/ ../data/split/amazon-google/ -npr 100 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-google: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T06:05:44.631086Z",
     "start_time": "2018-04-02T06:05:11.913119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 matched pairs detected. Creating 130000 non-matched pairs.\n",
      "Training set contains 105040 instances\n",
      "Validation set contains 13130 instances\n",
      "Test set contains 13130 instances\n",
      "Creating destination directory.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/amazon-google-reduced/ ../data/split/amazon-google-reduced/ -npr 100 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-walmart: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:13:04.743281Z",
     "start_time": "2018-04-12T21:12:32.411484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154 matched pairs detected. Creating 115400 non-matched pairs.\n",
      "Training set contains 93243 instances\n",
      "Validation set contains 11656 instances\n",
      "Test set contains 11655 instances\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/amazon-walmart/ ../data/split/amazon-walmart/ -npr 100 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon-walmart: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:38:29.444136Z",
     "start_time": "2018-04-12T21:37:59.571849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154 matched pairs detected. Creating 115400 non-matched pairs.\n",
      "Training set contains 93243 instances\n",
      "Validation set contains 11656 instances\n",
      "Test set contains 11655 instances\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/amazon-walmart-reduced/ ../data/split/amazon-walmart-reduced/ -npr 100 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBLP-scholar: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T20:17:14.998876Z",
     "start_time": "2018-04-15T20:15:22.029627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5347 matched pairs detected. Creating 534700 non-matched pairs.\n",
      "Training set contains 432038 instances\n",
      "Validation set contains 54004 instances\n",
      "Test set contains 54005 instances\n",
      "Creating destination directory.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/dblp-scholar/ ../data/split/dblp-scholar/ -npr 100 -v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBLP-scholar: reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T20:19:56.088359Z",
     "start_time": "2018-04-15T20:17:39.219226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5347 matched pairs detected. Creating 534700 non-matched pairs.\n",
      "Training set contains 432038 instances\n",
      "Validation set contains 54004 instances\n",
      "Test set contains 54005 instances\n",
      "Creating destination directory.\n"
     ]
    }
   ],
   "source": [
    "! source activate entity-resolution && python make-splits.py ../data/converted/dblp-scholar-reduced/ ../data/split/dblp-scholar-reduced/ -npr 100 -v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:03:32.971125Z",
     "start_time": "2018-04-12T21:03:32.963284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make-splits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make-splits.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse as ap\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "parser = ap.ArgumentParser()\n",
    "parser.add_argument('source_dir',\n",
    "                    help='directory containing dataset and match files to split')\n",
    "parser.add_argument('dest_dir',\n",
    "                    help='directory to save split dataset csvs')\n",
    "parser.add_argument('--set1', '-s1', default='set1.csv',\n",
    "                    help='filename of first dataset csv')\n",
    "parser.add_argument('--set2', '-s2', default='set2.csv',\n",
    "                    help='filename of second dataset csv')\n",
    "parser.add_argument('--matches', '-m', default='matches.csv',\n",
    "                    help='filename of positives matches csv')\n",
    "parser.add_argument('--neg_pos_ratio', '-npr', default = 9, type=float,\n",
    "                    help='ratio of non-matching pairs to matching pairs')\n",
    "parser.add_argument('--val_prop', '-vp', default = 0.1, type=float,\n",
    "                    help='proportion of data to allocate to validation set')\n",
    "parser.add_argument('--test_prop', '-tp', default = 0.1, type=float,\n",
    "                    help='proportion of data to allocate to test set')\n",
    "parser.add_argument('--verbose', '-v', action='store_true',\n",
    "                    help='print statistics')\n",
    "\n",
    "# parse arguments\n",
    "args = parser.parse_args()\n",
    "source_dir = args.source_dir\n",
    "set1 = args.set1\n",
    "set2 = args.set2\n",
    "matches = args.matches\n",
    "\n",
    "destination_path = args.dest_dir\n",
    "\n",
    "neg_pos_ratio = args.neg_pos_ratio\n",
    "val_prop = args.val_prop\n",
    "test_prop = args.test_prop\n",
    "\n",
    "verbose = args.verbose\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(source_dir, set1), encoding = \"latin1\")\n",
    "df1['id1'] = df1['id1'].astype(str)\n",
    "\n",
    "df2 = pd.read_csv(os.path.join(source_dir, set2), encoding = \"latin1\")\n",
    "df2['id2'] = df2['id2'].astype(str)\n",
    "\n",
    "df_pos = pd.read_csv(os.path.join(source_dir, matches), encoding = \"latin1\")\n",
    "df_pos['id1'] = df_pos['id1'].astype(str)\n",
    "df_pos['id2'] = df_pos['id2'].astype(str)\n",
    "\n",
    "# calculate number of matches available and...\n",
    "# number of non-matches that need to be sampled\n",
    "n_positives = len(df_pos)\n",
    "n_negatives = int(neg_pos_ratio * n_positives)\n",
    "if verbose:\n",
    "    print('{} matched pairs detected. Creating {} non-matched pairs.'.\\\n",
    "          format(n_positives, n_negatives))\n",
    "    \n",
    "# extract id columns from respective datasets\n",
    "id1 = df1['id1'].astype(str)\n",
    "id2 = df2['id2'].astype(str)\n",
    "\n",
    "# create a mapping from id1 to a list of matches in id2.\n",
    "# when creating non-matches, we can consult dictmap to ensure non-matches...\n",
    "# are not accidentally constructed from matched pairs\n",
    "pos_map = defaultdict(list)\n",
    "for row in df_pos.iterrows():\n",
    "    id1_val = row[1]['id1']\n",
    "    id2_val = row[1]['id2']\n",
    "    pos_map[id1_val].append(id2_val)\n",
    "\n",
    "def drop_positives(df_negs, pos_map):\n",
    "    \"drops positive matches from dataframe of non-matches\"\n",
    "    df_negs = df_negs.copy()\n",
    "    for index, row in df_negs.iterrows():\n",
    "        if row['id2'] in pos_map[row['id1']]:\n",
    "            df_negs.drop(index, inplace=True)\n",
    "    return df_negs\n",
    "\n",
    "# create a set of [n_negatives] non-matches by sampling from [id1] and [id2]...\n",
    "# with replacement. because dropping duplicates and matches results in a...\n",
    "# lower count, we oversample then filter. if oversampling not sufficient...\n",
    "# repeat process with progressively larger oversampling multipliers until\n",
    "# [> n_negatives] final non-matches achieved.\n",
    "\n",
    "oversample_base = 1.2\n",
    "oversample_exp = 0\n",
    "df_negs = pd.DataFrame(columns = ['id1', 'id2'])\n",
    "\n",
    "while len(df_negs) < n_negatives:\n",
    "    oversample_exp += 1\n",
    "    oversample_multiplier = oversample_base ** oversample_exp\n",
    "    n_negatives_os = int(oversample_multiplier * n_negatives)\n",
    "    \n",
    "    id1_negs = id1.sample(n_negatives_os, replace=True).reset_index(drop=True)\n",
    "    id2_negs = id2.sample(n_negatives_os, replace=True).reset_index(drop=True)\n",
    "    \n",
    "    df_negs = pd.concat([id1_negs, id2_negs], axis = 'columns')\n",
    "    df_negs = df_negs.drop_duplicates().reset_index(drop=True)\n",
    "    df_negs = drop_positives(df_negs, pos_map)\n",
    "    df_negs = df_negs.iloc[:n_negatives,:]\n",
    "    \n",
    "# ensure all generated non-match pairs are not matches\n",
    "for index, row in df_negs.iterrows():\n",
    "    assert((row['id2'] in pos_map[row['id1']]) == False)\n",
    "\n",
    "# add target column to both positive and negative sets\n",
    "df_negs['match'] = 0\n",
    "df_pos['match'] = 1\n",
    "\n",
    "# vertically stack dataframes and shuffle\n",
    "df = pd.concat([df_negs, df_pos], axis = 'rows')\n",
    "df = df.sample(len(df), replace=False)\n",
    "\n",
    "# ensure all pairs in df are labeled correctly\n",
    "for index, row in df.iterrows():\n",
    "    assert((row['id2'] in pos_map[row['id1']]) == row['match'])\n",
    "\n",
    "# calculate indices on which to split dataset into test and validation sets\n",
    "test_idx = np.round(len(df) * test_prop).astype(int)\n",
    "val_idx = np.round(len(df) * (test_prop + val_prop)).astype(int)\n",
    "\n",
    "df_test = df.iloc[:test_idx,:]\n",
    "df_val = df.iloc[test_idx:val_idx,:]\n",
    "df_train = df.iloc[val_idx:,:]\n",
    "\n",
    "# merge in relevant attributes from each dataset to id's in train, val, ...\n",
    "# and test set\n",
    "\n",
    "df_train_1 = pd.merge(df_train, df1, how='left',  on=['id1'])\n",
    "df_train_1 = df_train_1.drop(['id2', 'match'], axis='columns')\n",
    "df_train_2 = pd.merge(df_train, df2, how='left',  on=['id2'])\n",
    "df_train_2 = df_train_2.drop(['id1', 'match'], axis='columns')\n",
    "df_train_y = df_train['match']\n",
    "\n",
    "df_val_1 = pd.merge(df_val, df1, how='left',  on=['id1'])\n",
    "df_val_1 = df_val_1.drop(['id2', 'match'], axis='columns')\n",
    "df_val_2 = pd.merge(df_val, df2, how='left',  on=['id2'])\n",
    "df_val_2 = df_val_2.drop(['id1', 'match'], axis='columns')\n",
    "df_val_y = df_val['match']\n",
    "\n",
    "df_test_1 = pd.merge(df_test, df1, how='left',  on=['id1'])\n",
    "df_test_1 = df_test_1.drop(['id2', 'match'], axis='columns')\n",
    "df_test_2 = pd.merge(df_test, df2, how='left',  on=['id2'])\n",
    "df_test_2 = df_test_2.drop(['id1', 'match'], axis='columns')\n",
    "df_test_y = df_test['match']\n",
    "\n",
    "# ensure all id's match\n",
    "assert(np.all(df_train_1['id1'].values == df_train['id1'].values))\n",
    "assert(np.all(df_train_2['id2'].values == df_train['id2'].values))\n",
    "assert(np.all(df_val_1['id1'].values == df_val['id1'].values))\n",
    "assert(np.all(df_val_2['id2'].values == df_val['id2'].values))\n",
    "assert(np.all(df_test_1['id1'].values == df_test['id1'].values))\n",
    "assert(np.all(df_test_2['id2'].values == df_test['id2'].values))\n",
    "\n",
    "if verbose:\n",
    "    print('Training set contains {} instances'.format(len(df_train_y)))\n",
    "    print('Validation set contains {} instances'.format(len(df_val_y)))\n",
    "    print('Test set contains {} instances'.format(len(df_test_y)))\n",
    "    \n",
    "if not os.path.isdir(destination_path):\n",
    "    os.mkdir(destination_path)\n",
    "    if verbose:\n",
    "        print('Creating destination directory.')\n",
    "        \n",
    "# convert 'y' Series to dataframes to avoid header import mismatches\n",
    "df_train_y = pd.DataFrame(df_train_y)\n",
    "df_val_y = pd.DataFrame(df_val_y)\n",
    "df_test_y = pd.DataFrame(df_test_y)\n",
    "\n",
    "# save newly split dataframes in specified destination\n",
    "df_train_1.to_csv(os.path.join(destination_path, 'train_1.csv'), index=False)\n",
    "df_train_2.to_csv(os.path.join(destination_path, 'train_2.csv'), index=False)\n",
    "df_train_y.to_csv(os.path.join(destination_path, 'train_y.csv'), index=False)\n",
    "\n",
    "df_val_1.to_csv(os.path.join(destination_path, 'val_1.csv'), index=False)\n",
    "df_val_2.to_csv(os.path.join(destination_path, 'val_2.csv'), index=False)\n",
    "df_val_y.to_csv(os.path.join(destination_path, 'val_y.csv'), index=False)\n",
    "\n",
    "df_test_1.to_csv(os.path.join(destination_path, 'test_1.csv'), index=False)\n",
    "df_test_2.to_csv(os.path.join(destination_path, 'test_2.csv'), index=False)\n",
    "df_test_y.to_csv(os.path.join(destination_path, 'test_y.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:01:17.185408Z",
     "start_time": "2018-04-12T21:01:14.074568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154 matched pairs detected. Creating 10386 non-matched pairs.\n",
      "Training set contains 9232 instances\n",
      "Validation set contains 1154 instances\n",
      "Test set contains 1154 instances\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse as ap\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# parse arguments\n",
    "source_dir = '../data/converted/amazon-walmart-reduced/'\n",
    "set1 = 'set1.csv'\n",
    "set2 = 'set2.csv'\n",
    "matches = 'matches.csv'\n",
    "\n",
    "destination_path = '../data/split/amazon-walmart-reduced/'\n",
    "\n",
    "neg_pos_ratio = 9\n",
    "val_prop = 0.1\n",
    "test_prop = 0.1\n",
    "\n",
    "verbose = True\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(source_dir, set1), encoding = \"latin1\")\n",
    "df1['id1'] = df1['id1'].astype(str)\n",
    "\n",
    "df2 = pd.read_csv(os.path.join(source_dir, set2), encoding = \"latin1\")\n",
    "df2['id2'] = df2['id2'].astype(str)\n",
    "\n",
    "df_pos = pd.read_csv(os.path.join(source_dir, matches), encoding = \"latin1\")\n",
    "df_pos['id1'] = df_pos['id1'].astype(str)\n",
    "df_pos['id2'] = df_pos['id2'].astype(str)\n",
    "\n",
    "# calculate number of matches available and...\n",
    "# number of non-matches that need to be sampled\n",
    "n_positives = len(df_pos)\n",
    "n_negatives = int(neg_pos_ratio * n_positives)\n",
    "if verbose:\n",
    "    print('{} matched pairs detected. Creating {} non-matched pairs.'.\\\n",
    "          format(n_positives, n_negatives))\n",
    "    \n",
    "# extract id columns from respective datasets\n",
    "id1 = df1['id1']\n",
    "id2 = df2['id2']\n",
    "\n",
    "df_pos\n",
    "\n",
    "# create a mapping from id1 to a list of matches in id2.\n",
    "# when creating non-matches, we can consult dictmap to ensure non-matches...\n",
    "# are not accidentally constructed from matched pairs\n",
    "pos_map = defaultdict(list)\n",
    "for row in df_pos.iterrows():\n",
    "    id1_val = row[1]['id1']\n",
    "    id2_val = row[1]['id2']\n",
    "    pos_map[id1_val].append(id2_val)\n",
    "\n",
    "def drop_positives(df_negs, pos_map):\n",
    "    \"drops positive matches from dataframe of non-matches\"\n",
    "    df_negs = df_negs.copy()\n",
    "    for index, row in df_negs.iterrows():\n",
    "        if row['id2'] in pos_map[row['id1']]:\n",
    "            df_negs.drop(index, inplace=True)\n",
    "    return df_negs\n",
    "\n",
    "# create a set of [n_negatives] non-matches by sampling from [id1] and [id2]...\n",
    "# with replacement. because dropping duplicates and matches results in a...\n",
    "# lower count, we oversample then filter. if oversampling not sufficient...\n",
    "# repeat process with progressively larger oversampling multipliers until\n",
    "# [> n_negatives] final non-matches achieved.\n",
    "\n",
    "oversample_base = 1.2\n",
    "oversample_exp = 0\n",
    "df_negs = pd.DataFrame(columns = ['id1', 'id2'])\n",
    "\n",
    "while len(df_negs) < n_negatives:\n",
    "    oversample_exp += 1\n",
    "    oversample_multiplier = oversample_base ** oversample_exp\n",
    "    n_negatives_os = int(oversample_multiplier * n_negatives)\n",
    "    \n",
    "    id1_negs = id1.sample(n_negatives_os, replace=True).reset_index(drop=True)\n",
    "    id2_negs = id2.sample(n_negatives_os, replace=True).reset_index(drop=True)\n",
    "    \n",
    "    df_negs = pd.concat([id1_negs, id2_negs], axis = 'columns')\n",
    "    df_negs = df_negs.drop_duplicates().reset_index(drop=True)\n",
    "    df_negs = drop_positives(df_negs, pos_map)\n",
    "    df_negs = df_negs.iloc[:n_negatives,:]\n",
    "    \n",
    "# ensure all generated non-match pairs are not matches\n",
    "for index, row in df_negs.iterrows():\n",
    "    assert((row['id2'] in pos_map[row['id1']]) == False)\n",
    "\n",
    "# add target column to both positive and negative sets\n",
    "df_negs['match'] = 0\n",
    "df_pos['match'] = 1\n",
    "\n",
    "# vertically stack dataframes and shuffle\n",
    "df = pd.concat([df_negs, df_pos], axis = 'rows')\n",
    "df = df.sample(len(df), replace=False)\n",
    "\n",
    "# ensure all pairs in df are labeled correctly\n",
    "for index, row in df.iterrows():\n",
    "    assert((row['id2'] in pos_map[row['id1']]) == row['match'])\n",
    "\n",
    "# calculate indices on which to split dataset into test and validation sets\n",
    "test_idx = np.round(len(df) * test_prop).astype(int)\n",
    "val_idx = np.round(len(df) * (test_prop + val_prop)).astype(int)\n",
    "\n",
    "df_test = df.iloc[:test_idx,:]\n",
    "df_val = df.iloc[test_idx:val_idx,:]\n",
    "df_train = df.iloc[val_idx:,:]\n",
    "\n",
    "# merge in relevant attributes from each dataset to id's in train, val, ...\n",
    "# and test set\n",
    "\n",
    "df_train_1 = pd.merge(df_train, df1, how='left',  on=['id1'])\n",
    "df_train_1 = df_train_1.drop(['id2', 'match'], axis='columns')\n",
    "df_train_2 = pd.merge(df_train, df2, how='left',  on=['id2'])\n",
    "df_train_2 = df_train_2.drop(['id1', 'match'], axis='columns')\n",
    "df_train_y = df_train['match']\n",
    "\n",
    "df_val_1 = pd.merge(df_val, df1, how='left',  on=['id1'])\n",
    "df_val_1 = df_val_1.drop(['id2', 'match'], axis='columns')\n",
    "df_val_2 = pd.merge(df_val, df2, how='left',  on=['id2'])\n",
    "df_val_2 = df_val_2.drop(['id1', 'match'], axis='columns')\n",
    "df_val_y = df_val['match']\n",
    "\n",
    "df_test_1 = pd.merge(df_test, df1, how='left',  on=['id1'])\n",
    "df_test_1 = df_test_1.drop(['id2', 'match'], axis='columns')\n",
    "df_test_2 = pd.merge(df_test, df2, how='left',  on=['id2'])\n",
    "df_test_2 = df_test_2.drop(['id1', 'match'], axis='columns')\n",
    "df_test_y = df_test['match']\n",
    "\n",
    "# ensure all id's match\n",
    "assert(np.all(df_train_1['id1'].values == df_train['id1'].values))\n",
    "assert(np.all(df_train_2['id2'].values == df_train['id2'].values))\n",
    "assert(np.all(df_val_1['id1'].values == df_val['id1'].values))\n",
    "assert(np.all(df_val_2['id2'].values == df_val['id2'].values))\n",
    "assert(np.all(df_test_1['id1'].values == df_test['id1'].values))\n",
    "assert(np.all(df_test_2['id2'].values == df_test['id2'].values))\n",
    "\n",
    "if verbose:\n",
    "    print('Training set contains {} instances'.format(len(df_train_y)))\n",
    "    print('Validation set contains {} instances'.format(len(df_val_y)))\n",
    "    print('Test set contains {} instances'.format(len(df_test_y)))\n",
    "    \n",
    "# if not os.path.isdir(destination_path):\n",
    "#     os.mkdir(destination_path)\n",
    "#     if verbose:\n",
    "#         print('Creating destination directory.')\n",
    "        \n",
    "# # convert 'y' Series to dataframes to avoid header import mismatches\n",
    "# df_train_y = pd.DataFrame(df_train_y)\n",
    "# df_val_y = pd.DataFrame(df_val_y)\n",
    "# df_test_y = pd.DataFrame(df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T21:37:34.378296Z",
     "start_time": "2018-04-12T21:37:33.307304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "df_converted = pd.read_csv('../data/converted/amazon-walmart-reduced/set1.csv')\n",
    "df_raw = pd.read_csv('../data/raw/amazon-walmart/set1.csv')\n",
    "\n",
    "with open('../data/converted/amazon-walmart-reduced/glove-300.map', 'rb') as f:\n",
    "    map = pkl.load(f)\n",
    "\n",
    "df_converted.brand.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-resolution",
   "language": "python",
   "name": "entity-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
